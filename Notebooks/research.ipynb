{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "611270ec",
   "metadata": {},
   "source": [
    "```\n",
    "User Query\n",
    "    ‚Üì\n",
    "Navigator Agent  ‚îÄ‚îÄ‚ñ∫ (file paths) ‚îÄ‚îÄ‚ñ∫ Fetcher Agent ‚îÄ‚îÄ‚ñ∫ (file content)\n",
    "                                                            ‚îÇ\n",
    "Chunker Agent ‚óÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "    ‚Üì\n",
    "Embedder Agent\n",
    "    ‚Üì\n",
    "ChromaDB\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a672e4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sumi/Desktop/GitSurfer/Notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c8966aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sumi/Desktop/GitSurfer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../\")\n",
    "\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "929ba55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b82336cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GEMINI_API_KEY\"]=os.getenv(\"GOOGLE_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27ccaafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import base64\n",
    "import json\n",
    "\n",
    "from typing import List, Tuple, Dict, Any, Optional, Annotated\n",
    "from operator import add\n",
    "from pydantic import BaseModel, Field\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from logger import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Auth headers\n",
    "GITHUB_TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "AUTH_HEADERS = {\n",
    "    \"Authorization\": f\"token {GITHUB_TOKEN}\",\n",
    "    \"Accept\": \"application/vnd.github.v3+json\"\n",
    "}\n",
    "\n",
    "# Retry helper\n",
    "async def retry_async(func, retries=3, backoff_in_seconds=1, *args, **kwargs):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return await func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Attempt {attempt+1} failed with error: {e}\")\n",
    "            if attempt == retries - 1:\n",
    "                raise\n",
    "            await asyncio.sleep(backoff_in_seconds * 2 ** attempt)\n",
    "\n",
    "# --- Node 1: Fetch Tree ---\n",
    "async def fetch_github_tree(owner: str, repo: str, branch: str = \"main\", session: Optional[aiohttp.ClientSession] = None) -> List[Tuple[str, str]]:\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/git/trees/{branch}?recursive=1\"\n",
    "    close_session = False\n",
    "    if session is None:\n",
    "        session = aiohttp.ClientSession()\n",
    "        close_session = True\n",
    "    try:\n",
    "        async with session.get(url, headers=AUTH_HEADERS) as response:\n",
    "            if response.status != 200:\n",
    "                text = await response.text()\n",
    "                raise Exception(f\"Error fetching tree: {response.status} ‚Äî {text}\")\n",
    "            data = await response.json()\n",
    "            return [(item[\"path\"], item[\"type\"]) for item in data[\"tree\"]]\n",
    "    finally:\n",
    "        if close_session:\n",
    "            await session.close()\n",
    "\n",
    "class FetchTreeNode(Runnable):\n",
    "    def invoke(self, state, config=None):\n",
    "        return asyncio.run(self.ainvoke(state, config))\n",
    "    \n",
    "    async def ainvoke(self, state, config=None):\n",
    "        owner = state.owner\n",
    "        repo = state.repo\n",
    "        branch = getattr(state, \"branch\", \"main\")\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            tree_data = await retry_async(fetch_github_tree, owner=owner, repo=repo, branch=branch, session=session)\n",
    "        logger.info(f\"Fetched tree with {len(tree_data)} items\")\n",
    "        return {\"tree\": tree_data}\n",
    "\n",
    "# --- Node 2: Process Tree with LLM ---\n",
    "class SummarizeTreeNode(Runnable):\n",
    "    def __init__(self):\n",
    "        self.llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "    def invoke(self, state, config=None):\n",
    "        return asyncio.run(self.ainvoke(state, config))\n",
    "    \n",
    "    async def ainvoke(self, state, config=None):\n",
    "        tree_items = state.tree\n",
    "        # Build tree text, limit length to avoid token overflow\n",
    "        tree_text = \"\\n\".join(f\"- {p}\" for p, t in tree_items)\n",
    "        max_length = 3000  # adjust based on token limits\n",
    "        if len(tree_text) > max_length:\n",
    "            tree_text = tree_text[:max_length] + \"\\n... (truncated)\"\n",
    "        prompt = f\"\"\"\n",
    "        You are a helpful assistant. Analyze the following list of file paths and produce a JSON structure \n",
    "        that represents the file/folder hierarchy. Do not include metadata, just the structure.\n",
    "        Ensure that you do not include ```json``` in your response.\n",
    "\n",
    "        FILE TREE:\n",
    "        {tree_text}\n",
    "        \"\"\"\n",
    "        response = await self.llm.ainvoke([HumanMessage(content=prompt)])\n",
    "        # Try to parse JSON safely\n",
    "        try:\n",
    "            tree_summary = json.loads(response.content)\n",
    "        except Exception:\n",
    "            logger.warning(\"LLM response is not valid JSON, saving raw content\")\n",
    "            tree_summary = response.content\n",
    "        with open(\"tree.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            if isinstance(tree_summary, str):\n",
    "                f.write(tree_summary)\n",
    "            else:\n",
    "                json.dump(tree_summary, f, indent=2)\n",
    "        return {\"tree_summary\": tree_summary}\n",
    "\n",
    "# --- Node 3: Fetch All Files in Parallel ---\n",
    "async def fetch_file_content(owner: str, repo: str, path: str, branch: str = \"main\", session: Optional[aiohttp.ClientSession] = None) -> Dict[str, Any]:\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{path}?ref={branch}\"\n",
    "    close_session = False\n",
    "    if session is None:\n",
    "        session = aiohttp.ClientSession()\n",
    "        close_session = True\n",
    "    try:\n",
    "        async with session.get(url, headers=AUTH_HEADERS) as resp:\n",
    "            if resp.status != 200:\n",
    "                logger.warning(f\"Failed to fetch {path}: HTTP {resp.status}\")\n",
    "                return {\"path\": path, \"error\": f\"{resp.status}\"}\n",
    "            data = await resp.json()\n",
    "            if data.get(\"encoding\") == \"base64\":\n",
    "                content = base64.b64decode(data[\"content\"]).decode(\"utf-8\", errors=\"ignore\")\n",
    "            else:\n",
    "                content = data.get(\"content\", \"\")\n",
    "            return {\"path\": path, \"content\": content}\n",
    "    finally:\n",
    "        if close_session:\n",
    "            await session.close()\n",
    "\n",
    "async def fetch_all_files_by_path(owner: str, repo: str, paths: List[str], branch: str = \"main\", max_concurrency: int = 10) -> List[Dict[str, Any]]:\n",
    "    sem = asyncio.Semaphore(max_concurrency)\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async def limited_fetch(path):\n",
    "            async with sem:\n",
    "                return await retry_async(fetch_file_content, owner=owner, repo=repo, path=path, branch=branch, session=session)\n",
    "        results = await asyncio.gather(*(limited_fetch(p) for p in paths))\n",
    "    # Filter out errors or empty content\n",
    "    filtered = [res for res in results if res.get(\"content\")]\n",
    "    logger.info(f\"Fetched {len(filtered)} files with content\")\n",
    "    return filtered\n",
    "\n",
    "class FetchFilesNode(Runnable):\n",
    "    def invoke(self, state, config=None):\n",
    "        return asyncio.run(self.ainvoke(state, config))\n",
    "    \n",
    "    async def ainvoke(self, state, config=None):\n",
    "        owner = state.owner\n",
    "        repo = state.repo\n",
    "        branch = getattr(state, \"branch\", \"main\")\n",
    "        paths = [p for p, t in state.tree if t == \"blob\"]\n",
    "        results = await fetch_all_files_by_path(owner, repo, paths, branch)\n",
    "        with open(\"chunks_raw.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        return {\"files\": results}\n",
    "\n",
    "# --- Define State Schema ---\n",
    "class MyState(BaseModel):\n",
    "    owner: str\n",
    "    repo: str\n",
    "    branch: str = \"main\"\n",
    "    tree: List[Tuple[str, str]] = Field(default_factory=list)\n",
    "    files: Annotated[List[Dict[str, Any]], add] = Field(default_factory=list)\n",
    "\n",
    "# --- Define LangGraph ---\n",
    "graph = StateGraph(state_schema=MyState)\n",
    "graph.add_node(\"fetch_tree\", FetchTreeNode())\n",
    "graph.add_node(\"summarize_tree\", SummarizeTreeNode())\n",
    "graph.add_node(\"fetch_files\", FetchFilesNode())\n",
    "\n",
    "# Parallel execution after tree fetch\n",
    "graph.add_edge(\"fetch_tree\", \"summarize_tree\")\n",
    "graph.add_edge(\"fetch_tree\", \"fetch_files\")\n",
    "\n",
    "graph.set_entry_point(\"fetch_tree\")\n",
    "graph.set_finish_point(\"fetch_files\")\n",
    "\n",
    "flow = graph.compile()\n",
    "#flow\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4296a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'owner': 'kumar8074', 'repo': 'NOVA-AI', 'branch': 'main', 'tree': [('.devcontainer', 'tree'), ('.devcontainer/devcontainer.json', 'blob'), ('.gitignore', 'blob'), ('LICENSE', 'blob'), ('Notebooks', 'tree'), ('Notebooks/1-nova-basic.ipynb', 'blob'), ('README.md', 'blob'), ('__init__.py', 'blob'), ('app.py', 'blob'), ('config.py', 'blob'), ('models', 'tree'), ('models/__init__.py', 'blob'), ('models/llm.py', 'blob'), ('nova.py', 'blob'), ('prompts', 'tree'), ('prompts/__init__.py', 'blob'), ('prompts/templates.py', 'blob'), ('requirements.txt', 'blob'), ('utils', 'tree'), ('utils/__init__.py', 'blob'), ('utils/document_utils.py', 'blob'), ('utils/search_utils.py', 'blob'), ('utils/ui_utils.py', 'blob'), ('voicebot.py', 'blob')], 'files': [{'path': '.devcontainer/devcontainer.json', 'content': '{\\n  \"name\": \"Python 3\",\\n  // Or use a Dockerfile or Docker Compose file. More info: https://containers.dev/guide/dockerfile\\n  \"image\": \"mcr.microsoft.com/devcontainers/python:1-3.11-bullseye\",\\n  \"customizations\": {\\n    \"codespaces\": {\\n      \"openFiles\": [\\n        \"README.md\",\\n        \"app.py\"\\n      ]\\n    },\\n    \"vscode\": {\\n      \"settings\": {},\\n      \"extensions\": [\\n        \"ms-python.python\",\\n        \"ms-python.vscode-pylance\"\\n      ]\\n    }\\n  },\\n  \"updateContentCommand\": \"[ -f packages.txt ] && sudo apt update && sudo apt upgrade -y && sudo xargs apt install -y <packages.txt; [ -f requirements.txt ] && pip3 install --user -r requirements.txt; pip3 install --user streamlit; echo \\'‚úÖ Packages installed and Requirements met\\'\",\\n  \"postAttachCommand\": {\\n    \"server\": \"streamlit run app.py --server.enableCORS false --server.enableXsrfProtection false\"\\n  },\\n  \"portsAttributes\": {\\n    \"8501\": {\\n      \"label\": \"Application\",\\n      \"onAutoForward\": \"openPreview\"\\n    }\\n  },\\n  \"forwardPorts\": [\\n    8501\\n  ]\\n}'}, {'path': '.gitignore', 'content': '.env\\n__pycache__'}, {'path': 'LICENSE', 'content': 'MIT License\\n\\nCopyright (c) 2025 kumar8074\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n'}, {'path': 'Notebooks/1-nova-basic.ipynb', 'content': '{\\n \"cells\": [\\n  {\\n   \"cell_type\": \"markdown\",\\n   \"metadata\": {},\\n   \"source\": [\\n    \"## NOVA IS IMPLEMENTED WITH DOCUMENT INTELLIGENCE\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 1,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"data\": {\\n      \"text/plain\": [\\n       \"True\"\\n      ]\\n     },\\n     \"execution_count\": 1,\\n     \"metadata\": {},\\n     \"output_type\": \"execute_result\"\\n    }\\n   ],\\n   \"source\": [\\n    \"# Import necessary libraries\\\\n\",\\n    \"import os\\\\n\",\\n    \"from dotenv import load_dotenv\\\\n\",\\n    \"from langchain_community.document_loaders import PDFPlumberLoader # Load the document\\\\n\",\\n    \"from langchain.text_splitter import RecursiveCharacterTextSplitter # split the documkent\\\\n\",\\n    \"from langchain_ollama import OllamaEmbeddings  # Embed the documents\\\\n\",\\n    \"from langchain_core.vectorstores import InMemoryVectorStore # store the data\\\\n\",\\n    \"from langchain_groq import ChatGroq\\\\n\",\\n    \"\\\\n\",\\n    \"from langchain_core.prompts import ChatPromptTemplate\\\\n\",\\n    \"from langchain.chains.combine_documents import create_stuff_documents_chain # For combining all the documents\\\\n\",\\n    \"from langchain.chains import create_retrieval_chain\\\\n\",\\n    \"from langchain_core.output_parsers import StrOutputParser\\\\n\",\\n    \"load_dotenv()\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 2,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"/Users/sumi/Desktop/NOVA/Notebooks\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"!pwd\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 3,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"name\": \"stdout\",\\n     \"output_type\": \"stream\",\\n     \"text\": [\\n      \"/Users/sumi/Desktop/NOVA\\\\n\"\\n     ]\\n    }\\n   ],\\n   \"source\": [\\n    \"os.chdir(\\'../\\')\\\\n\",\\n    \"!pwd\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 4,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"# Get the API Keys\\\\n\",\\n    \"groq_api_key=os.getenv(\\\\\"GROQ_API_KEY\\\\\")\\\\n\",\\n    \"tavily_api_key=os.getenv(\\\\\"TAVILY_API_KEY\\\\\")\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 5,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": [\\n    \"# Load the document\\\\n\",\\n    \"loader=PDFPlumberLoader(\\'/Users/sumi/Desktop/NOVA/HSI-project-document.pdf\\')\\\\n\",\\n    \"raw_docs=loader.load()\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 6,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"data\": {\\n      \"text/plain\": [\\n       \"[Document(metadata={\\'source\\': \\'/Users/sumi/Desktop/NOVA/HSI-project-document.pdf\\', \\'file_path\\': \\'/Users/sumi/Desktop/NOVA/HSI-project-document.pdf\\', \\'page\\': 0, \\'total_pages\\': 3, \\'Title\\': \\'HSI-project-document\\', \\'Producer\\': \\'Skia/PDF m134 Google Docs Renderer\\', \\'start_index\\': 0}, page_content=\\'HyperSpectral Image Classification\\\\\\\\nProject overview: A HyperSpectral Image(HSI) is a high-dimensionality\\\\\\\\ndataset that captures images across numerous spectral bands, beyond the\\\\\\\\nvisible spectrum. Unlike RGB images (3 bands), HSI consists of hundreds\\\\\\\\nof contiguous bands spanning in ultraviolet, visible, infrared and beyond\\\\\\\\nregions.\\\\\\\\nThis High-dimensional nature of HSI data makes it challenging for\\\\\\\\ntraditional machine learning models to capture intricate spectral-spatial\\\\\\\\nrelationships. Hence we propose deep learning based approaches to\\\\\\\\naccurately classify HSI data.\\'),\\\\n\",\\n       \" Document(metadata={\\'source\\': \\'/Users/sumi/Desktop/NOVA/HSI-project-document.pdf\\', \\'file_path\\': \\'/Users/sumi/Desktop/NOVA/HSI-project-document.pdf\\', \\'page\\': 1, \\'total_pages\\': 3, \\'Title\\': \\'HSI-project-document\\', \\'Producer\\': \\'Skia/PDF m134 Google Docs Renderer\\', \\'start_index\\': 0}, page_content=\\'These workflows when run independently work fine and produce expected\\\\\\\\nresults.\\\\\\\\nThe Problem: The key challenge in integrating this project into an MLOps\\\\\\\\npipeline arises from the AutoEncoder-based approach. The\\\\\\\\nAutoEncoder is utilized for dimensionality reduction, making it part of the\\\\\\\\ndata_transformation step. However, for dimensionality reduction to be\\\\\\\\neffective, the AutoEncoder must first be trained, which is typically done in\\\\\\\\nthe model_trainer step.\\\\\\\\nThis creates a circular dependency:\\\\\\\\n‚óè The model_trainer step needs to be executed before\\\\\\\\ndata_transformation, so the AutoEncoder can learn meaningful\\\\\\\\nrepresentations.\\'),\\\\n\",\\n       \" Document(metadata={\\'source\\': \\'/Users/sumi/Desktop/NOVA/HSI-project-document.pdf\\', \\'file_path\\': \\'/Users/sumi/Desktop/NOVA/HSI-project-document.pdf\\', \\'page\\': 2, \\'total_pages\\': 3, \\'Title\\': \\'HSI-project-document\\', \\'Producer\\': \\'Skia/PDF m134 Google Docs Renderer\\', \\'start_index\\': 0}, page_content=\\'‚óè After transformation, the model_trainer step needs to be executed\\\\\\\\nagain to fine-tune the classification model using the transformed data.\\\\\\\\nData Ingestion --> Model Training (AutoEncoder) --> Data Transformation\\\\\\\\n--> Model Training (AutoEncoder+Classifier) --> Prediction\\\\\\\\nProject structure:\\\\\\\\nHyperSpec-AI\\\\\\\\n‚îÇ-- config/ # Configuration files (e.g., paths, parameters)\\\\\\\\n‚îÇ ‚îî‚îÄ‚îÄ config.yaml\\\\\\\\n‚îÇ-- DATA/ # Dataset storage\\\\\\\\n‚îÇ-- Notebooks/ # Jupyter Notebooks for research & experimentation\\\\\\\\n‚îÇ-- src/\\\\\\\\n‚îÇ ‚îú‚îÄ‚îÄ components/ # Core components of the pipeline\\\\\\\\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ data_ingestion.py # Handles dataset loading\\\\\\\\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ data_transformation.py # Applies transformations (e.g.,\\\\\\\\nAutoEncoder)\\\\\\\\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ model_trainer.py # Trains models (CNN, AutoEncoder,\\\\\\\\nClassifier)\\\\\\\\n‚îÇ ‚îú‚îÄ‚îÄ models/ # Model architectures\\\\\\\\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ cnn_model.py # Convolutional Neural Network\\\\\\\\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ autoencoder_classifier_head.py # AutoEncoder + Classifier\\\\\\\\n‚îÇ ‚îú‚îÄ‚îÄ pipeline/ # End-to-end pipeline execution\\\\\\\\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ training_pipeline.py # Full training pipeline\\'),\\\\n\",\\n       \" Document(metadata={\\'source\\': \\'/Users/sumi/Desktop/NOVA/HSI-project-document.pdf\\', \\'file_path\\': \\'/Users/sumi/Desktop/NOVA/HSI-project-document.pdf\\', \\'page\\': 2, \\'total_pages\\': 3, \\'Title\\': \\'HSI-project-document\\', \\'Producer\\': \\'Skia/PDF m134 Google Docs Renderer\\', \\'start_index\\': 825}, page_content=\\'‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ autoencoder_classifier_head.py # AutoEncoder + Classifier\\\\\\\\n‚îÇ ‚îú‚îÄ‚îÄ pipeline/ # End-to-end pipeline execution\\\\\\\\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ training_pipeline.py # Full training pipeline\\\\\\\\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ predict_pipeline.py # Inference pipeline\\\\\\\\n‚îÇ ‚îî‚îÄ‚îÄ utils.py # Utility functions (logging, preprocessing, etc.)\\\\\\\\nThe logs and artifacts will be automatically generated.\\\\\\\\nPlease read the logs for better understandability of the project flow.\\')]\"\\n      ]\\n     },\\n     \"execution_count\": 6,\\n     \"metadata\": {},\\n     \"output_type\": \"execute_result\"\\n    }\\n   ],\\n   \"source\": [\\n    \"# Split the document\\\\n\",\\n    \"text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,\\\\n\",\\n    \"                                              chunk_overlap=200,\\\\n\",\\n    \"                                              add_start_index=True)\\\\n\",\\n    \"\\\\n\",\\n    \"document_chunks=text_splitter.split_documents(raw_docs)\\\\n\",\\n    \"document_chunks\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 7,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"data\": {\\n      \"text/plain\": [\\n       \"<langchain_core.vectorstores.in_memory.InMemoryVectorStore at 0x10d37bdc0>\"\\n      ]\\n     },\\n     \"execution_count\": 7,\\n     \"metadata\": {},\\n     \"output_type\": \"execute_result\"\\n    }\\n   ],\\n   \"source\": [\\n    \"# Embed the documents and store in vector store\\\\n\",\\n    \"EMBEDDING_MODEL = OllamaEmbeddings(model=\\\\\"deepseek-r1:1.5b\\\\\")\\\\n\",\\n    \"vector_store=InMemoryVectorStore.from_documents(document_chunks,EMBEDDING_MODEL) # Any DB like FAISS, CHROMA, ASTRA can be used\\\\n\",\\n    \"vector_store\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 8,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"data\": {\\n      \"text/plain\": [\\n       \"\\'‚óè After transformation, the model_trainer step needs to be executed\\\\\\\\nagain to fine-tune the classification model using the transformed data.\\\\\\\\nData Ingestion --> Model Training (AutoEncoder) --> Data Transformation\\\\\\\\n--> Model Training (AutoEncoder+Classifier) --> Prediction\\\\\\\\nProject structure:\\\\\\\\nHyperSpec-AI\\\\\\\\n‚îÇ-- config/ # Configuration files (e.g., paths, parameters)\\\\\\\\n‚îÇ ‚îî‚îÄ‚îÄ config.yaml\\\\\\\\n‚îÇ-- DATA/ # Dataset storage\\\\\\\\n‚îÇ-- Notebooks/ # Jupyter Notebooks for research & experimentation\\\\\\\\n‚îÇ-- src/\\\\\\\\n‚îÇ ‚îú‚îÄ‚îÄ components/ # Core components of the pipeline\\\\\\\\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ data_ingestion.py # Handles dataset loading\\\\\\\\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ data_transformation.py # Applies transformations (e.g.,\\\\\\\\nAutoEncoder)\\\\\\\\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ model_trainer.py # Trains models (CNN, AutoEncoder,\\\\\\\\nClassifier)\\\\\\\\n‚îÇ ‚îú‚îÄ‚îÄ models/ # Model architectures\\\\\\\\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ cnn_model.py # Convolutional Neural Network\\\\\\\\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ autoencoder_classifier_head.py # AutoEncoder + Classifier\\\\\\\\n‚îÇ ‚îú‚îÄ‚îÄ pipeline/ # End-to-end pipeline execution\\\\\\\\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ training_pipeline.py # Full training pipeline\\'\"\\n      ]\\n     },\\n     \"execution_count\": 8,\\n     \"metadata\": {},\\n     \"output_type\": \"execute_result\"\\n    }\\n   ],\\n   \"source\": [\\n    \"# Query the vector DB\\\\n\",\\n    \"query=\\\\\"what is the major problem\\\\\"\\\\n\",\\n    \"result=vector_store.similarity_search(query,k=4)\\\\n\",\\n    \"result[0].page_content\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 9,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"data\": {\\n      \"text/plain\": [\\n       \"\\'‚óè After transformation, the model_trainer step needs to be executed\\\\\\\\nagain to fine-tune the classification model using the transformed data.\\\\\\\\nData Ingestion --> Model Training (AutoEncoder) --> Data Transformation\\\\\\\\n--> Model Training (AutoEncoder+Classifier) --> Prediction\\\\\\\\nProject structure:\\\\\\\\nHyperSpec-AI\\\\\\\\n‚îÇ-- config/ # Configuration files (e.g., paths, parameters)\\\\\\\\n‚îÇ ‚îî‚îÄ‚îÄ config.yaml\\\\\\\\n‚îÇ-- DATA/ # Dataset storage\\\\\\\\n‚îÇ-- Notebooks/ # Jupyter Notebooks for research & experimentation\\\\\\\\n‚îÇ-- src/\\\\\\\\n‚îÇ ‚îú‚îÄ‚îÄ components/ # Core components of the pipeline\\\\\\\\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ data_ingestion.py # Handles dataset loading\\\\\\\\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ data_transformation.py # Applies transformations (e.g.,\\\\\\\\nAutoEncoder)\\\\\\\\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ model_trainer.py # Trains models (CNN, AutoEncoder,\\\\\\\\nClassifier)\\\\\\\\n‚îÇ ‚îú‚îÄ‚îÄ models/ # Model architectures\\\\\\\\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ cnn_model.py # Convolutional Neural Network\\\\\\\\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ autoencoder_classifier_head.py # AutoEncoder + Classifier\\\\\\\\n‚îÇ ‚îú‚îÄ‚îÄ pipeline/ # End-to-end pipeline execution\\\\\\\\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ training_pipeline.py # Full training pipeline\\'\"\\n      ]\\n     },\\n     \"execution_count\": 9,\\n     \"metadata\": {},\\n     \"output_type\": \"execute_result\"\\n    }\\n   ],\\n   \"source\": [\\n    \"# Convert it into retriever and query\\\\n\",\\n    \"retriever=vector_store.as_retriever()\\\\n\",\\n    \"result=retriever.invoke(query)\\\\n\",\\n    \"result[0].page_content\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 10,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"data\": {\\n      \"text/plain\": [\\n       \"AIMessage(content=\\'<think>\\\\\\\\n\\\\\\\\n</think>\\\\\\\\n\\\\\\\\nGenerative AI refers to a type of artificial intelligence that can create or generate new content, such as text, images, music, or even code. Unlike traditional AI, which is typically designed to perform specific tasks or make predictions, generative AI models are capable of producing outputs that are similar to those created by humans.\\\\\\\\n\\\\\\\\n### Key Characteristics of Generative AI:\\\\\\\\n1. **Content Creation**: Generative AI can produce new content, such as writing articles, composing music, or generating images.\\\\\\\\n2. **Learning from Data**: These models are trained on large datasets to understand patterns, styles, and structures.\\\\\\\\n3. **Creativity**: They can combine existing information in novel ways to create something unique.\\\\\\\\n4. **Applications**: Generative AI is used in various fields, including art, entertainment, education, and business.\\\\\\\\n\\\\\\\\n### Examples of Generative AI:\\\\\\\\n- **Text Generation**: Chatbots, automated essay writers, and language translation tools.\\\\\\\\n- **Image Generation**: Tools like DALL-E, MidJourney, and Stable Diffusion, which create images from text prompts.\\\\\\\\n- **Music and Sound**: AI-generated music, sound effects, and even entire compositions.\\\\\\\\n- **Code Generation**: Tools that help developers write code by suggesting snippets or completing functions.\\\\\\\\n\\\\\\\\n### How Generative AI Works:\\\\\\\\nGenerative AI often uses machine learning models, particularly **generative adversarial networks (GANs)** and **transformers**. These models are trained on vast amounts of data to learn patterns and then generate new outputs by sampling from the learned distribution.\\\\\\\\n\\\\\\\\n### Applications of Generative AI:\\\\\\\\n1. **Art and Design**: Creating digital art, designing products, and generating 3D models.\\\\\\\\n2. **Entertainment**: Writing scripts, composing music, and producing videos.\\\\\\\\n3. **Education**: Generating educational content, quizzes, and personalized learning materials.\\\\\\\\n4. **Marketing**: Creating ads, social media posts, and product descriptions.\\\\\\\\n5. **Healthcare**: Generating synthetic medical data for research and training purposes.\\\\\\\\n\\\\\\\\n### Ethical Considerations:\\\\\\\\n- **Bias**: Generative AI can inherit biases from its training data, leading to unfair or misleading outputs.\\\\\\\\n- **Misuse**: The technology can be used maliciously, such as creating deepfakes or spreading misinformation.\\\\\\\\n- **Copyright and Ownership**: Questions arise about who owns the rights to AI-generated content.\\\\\\\\n\\\\\\\\nGenerative AI is a rapidly evolving field with immense potential to transform industries, but it also raises important ethical and societal questions that need to be addressed as the technology advances.\\', additional_kwargs={}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 508, \\'prompt_tokens\\': 7, \\'total_tokens\\': 515, \\'completion_time\\': 3.628571429, \\'prompt_time\\': 0.002811103, \\'queue_time\\': 0.057204238, \\'total_time\\': 3.631382532}, \\'model_name\\': \\'Deepseek-R1-Distill-Qwen-32b\\', \\'system_fingerprint\\': \\'fp_0852292947\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-d3ca16dc-1c14-46cd-8046-d008dc6f84ea-0\\', usage_metadata={\\'input_tokens\\': 7, \\'output_tokens\\': 508, \\'total_tokens\\': 515})\"\\n      ]\\n     },\\n     \"execution_count\": 10,\\n     \"metadata\": {},\\n     \"output_type\": \"execute_result\"\\n    }\\n   ],\\n   \"source\": [\\n    \"# Setup the llm\\\\n\",\\n    \"llm_engine = ChatGroq(model=\\\\\"Deepseek-R1-Distill-Qwen-32b\\\\\", groq_api_key=groq_api_key)\\\\n\",\\n    \"# provide sample input and get sample response from the LLM\\\\n\",\\n    \"result=llm_engine.invoke(\\\\\"what is generative AI\\\\\")\\\\n\",\\n    \"result\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": 13,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"data\": {\\n      \"text/plain\": [\\n       \"\\'<think>\\\\\\\\nOkay, so I\\\\\\\\\\'m trying to understand what LangSmith is. I know it\\\\\\\\\\'s related to machine learning and AI, but beyond that, I\\\\\\\\\\'m a bit fuzzy. Let me start by breaking down the name: \\\\\"Lang\\\\\" probably stands for language, and \\\\\"Smith\\\\\" is a common last name, but in tech, sometimes it\\\\\\\\\\'s used as a tool or framework name. Maybe it\\\\\\\\\\'s a tool for working with languages or something related.\\\\\\\\n\\\\\\\\nI remember that there\\\\\\\\\\'s a company called Hugging Face, which is known for their work in natural language processing (NLP). They have tools like Hugging Face Transformers, which are used for training and deploying NLP models. Maybe LangSmith is related to them? Or perhaps it\\\\\\\\\\'s an open-source project.\\\\\\\\n\\\\\\\\nI think I\\\\\\\\\\'ve heard of LangSmith in the context of creating and managing machine learning pipelines. Pipelines are sequences of data processing steps, right? So LangSmith might help in setting up these pipelines for different tasks, maybe not just NLP but other types of models too.\\\\\\\\n\\\\\\\\nWait, the user mentioned something about an open-source framework. So LangSmith could be a tool that allows developers to build, train, and deploy machine learning models efficiently. It might offer features like model management, data preprocessing, hyperparameter tuning, and integration with various ML frameworks.\\\\\\\\n\\\\\\\\nI should consider what problems LangSmith solves. Maybe it\\\\\\\\\\'s about simplifying the process of setting up machine learning experiments, making it easier to iterate and test different models. Or perhaps it provides a user-friendly interface for non-technical users to work with ML models.\\\\\\\\n\\\\\\\\nI\\\\\\\\\\'m also thinking about how LangSmith might fit into the broader AI ecosystem. Does it integrate with popular libraries like TensorFlow, PyTorch, or scikit-learn? That would make it more versatile for developers who use these tools.\\\\\\\\n\\\\\\\\nAnother angle is whether LangSmith has specific features for deployment. For example, maybe it helps in deploying models to production environments, handling scaling, monitoring, and updates. That would be valuable for teams looking to move beyond experimentation to actual deployment.\\\\\\\\n\\\\\\\\nI should also consider any tutorials or documentation available. If LangSmith is widely used, there should be resources to help users get started. Maybe there are examples of how to use LangSmith for specific tasks, like image classification or text generation.\\\\\\\\n\\\\\\\\nI\\\\\\\\\\'m a bit confused about whether LangSmith is a standalone tool or part of a larger platform. If it\\\\\\\\\\'s standalone, what makes it different from other tools like MLflow or Kubeflow? If it\\\\\\\\\\'s part of a platform, how does it complement other tools?\\\\\\\\n\\\\\\\\nI also wonder about the community around LangSmith. Is it actively maintained? Are there regular updates and improvements? A strong community can make a tool more reliable and easier to use.\\\\\\\\n\\\\\\\\nIn summary, LangSmith seems to be a tool related to machine learning, possibly for building and managing ML pipelines, integrating with other libraries, and simplifying the deployment process. It might be an open-source project aimed at making ML workflows more efficient. To get a clearer picture, I should look up the official documentation or any tutorials that explain its features and use cases.\\\\\\\\n\\\\\\\\nWait, I think I might have confused LangSmith with another tool. Let me check my thoughts again. I recall that Hugging Face has a tool called LangSmith, which is designed for creating and managing machine learning applications. It might offer features like model versioning, experiment tracking, and deployment capabilities. This would help data scientists and engineers streamline their ML workflows from development to production.\\\\\\\\n\\\\\\\\nSo, LangSmith could be a comprehensive platform that handles various stages of the ML lifecycle, making it easier for teams to collaborate and manage their projects effectively. It might also provide a user interface for monitoring models and handling data, which would be beneficial for both technical and non-technical users.\\\\\\\\n\\\\\\\\nI should also consider any recent updates or features. For example, does LangSmith support the latest machine learning techniques or frameworks? Is it compatible with cloud services for scalable deployment?\\\\\\\\n\\\\\\\\nIn conclusion, LangSmith is likely a tool designed to simplify and streamline the process of building, training, and deploying machine learning models, possibly with a focus on integration and ease of use. It might be part of a larger ecosystem, offering a range of features to support the entire ML workflow.\\\\\\\\n</think>\\\\\\\\n\\\\\\\\nLangSmith is a tool designed to simplify and streamline the process of building, training, and deploying machine learning models. It is likely part of a larger ecosystem, possibly associated with Hugging Face, and is focused on creating and managing machine learning applications. LangSmith may offer features such as model versioning, experiment tracking, deployment capabilities, and integration with various machine learning frameworks like TensorFlow, PyTorch, and scikit-learn. It aims to support the entire ML workflow, from development to production, and may provide a user interface for monitoring models and handling data, making it beneficial for both technical and non-technical users.\\'\"\\n      ]\\n     },\\n     \"execution_count\": 13,\\n     \"metadata\": {},\\n     \"output_type\": \"execute_result\"\\n    }\\n   ],\\n   \"source\": [\\n    \"# Create a sample chain using chatprompt templates and LCEL\\\\n\",\\n    \"prompt=ChatPromptTemplate.from_messages(\\\\n\",\\n    \"    [\\\\n\",\\n    \"        (\\\\\"system\\\\\", \\\\\"You are an Expert AI Engineer. Provide me the answer based on the questions\\\\\"),\\\\n\",\\n    \"        (\\\\\"user\\\\\", \\\\\"{input}\\\\\")\\\\n\",\\n    \"    ]\\\\n\",\\n    \")\\\\n\",\\n    \"\\\\n\",\\n    \"output_parser=StrOutputParser()\\\\n\",\\n    \"\\\\n\",\\n    \"chain= prompt | llm_engine | output_parser\\\\n\",\\n    \"\\\\n\",\\n    \"response= chain.invoke({\\\\\"input\\\\\": \\\\\"can you tell me about LangSmith\\\\\"})\\\\n\",\\n    \"response\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [\\n    {\\n     \"data\": {\\n      \"text/plain\": [\\n       \"\\\\\"<think>\\\\\\\\nOkay, I need to answer the user\\'s questions based on the provided context. Let me read through the context again to make sure I understand everything.\\\\\\\\n\\\\\\\\nThe project is about hyperpectral image classification using deep learning. They have a pipeline that includes data ingestion, model training with an AutoEncoder, data transformation using that AutoEncoder, further model training with both the AutoEncoder and a classifier, and finally prediction.\\\\\\\\n\\\\\\\\nThe structure of the project includes several files and folders, like config, DATA, Notebooks, src, and utils. The src folder has components for data ingestion, transformation, and model training, as well as models and pipeline scripts.\\\\\\\\n\\\\\\\\nThe problem they\\'re facing is a circular dependency. The model_trainer step needs to run before data_transformation because the AutoEncoder has to be trained to transform the data. But in their current setup, the model_trainer is part of the pipeline, and the data_transformation is a separate step that uses the trained model. So they can\\'t run model_trainer without the data being transformed, but the data can\\'t be transformed without the model being trained first.\\\\\\\\n\\\\\\\\nI think the user\\'s questions will revolve around understanding this circular dependency, how it affects the pipeline, and possible solutions. They might ask about the project structure, the workflow, the problem they\\'re facing, and how to resolve it.\\\\\\\\n\\\\\\\\nLet me outline potential questions and answers based on the context.\\\\\\\\n\\\\\\\\n1. What is the main challenge in integrating the project into an MLOps pipeline?\\\\\\\\n   - The main challenge is the circular dependency between model training and data transformation. The AutoEncoder needs to be trained before data transformation, but the data transformation is part of the pipeline that comes after model training.\\\\\\\\n\\\\\\\\n2. How does the AutoEncoder contribute to dimensionality reduction?\\\\\\\\n   - The AutoEncoder is trained to learn a compressed representation of the high-dimensional HSI data. Once trained, it\\'s used in the data transformation step to reduce the dimensionality of the data, making it easier for the classifier to process.\\\\\\\\n\\\\\\\\n3. What steps are involved in the project\\'s workflow?\\\\\\\\n   - The workflow includes Data Ingestion, Model Training (AutoEncoder), Data Transformation using the AutoEncoder, further Model Training (AutoEncoder + Classifier), and Prediction.\\\\\\\\n\\\\\\\\n4. How is the project structured?\\\\\\\\n   - The project has several key directories: config, DATA, Notebooks, src, and utils. The src directory includes components for data handling, model training, and the pipeline, along with model architectures and utility functions.\\\\\\\\n\\\\\\\\n5. Why is the circular dependency a problem?\\\\\\\\n   - Because the model_trainer needs to run before data_transformation, but in the pipeline, data_transformation is a step that comes after model_trainer. This creates a loop where each step depends on the other.\\\\\\\\n\\\\\\\\n6. What are the possible solutions to break the circular dependency?\\\\\\\\n   - Possible solutions might include training the AutoEncoder separately before the main pipeline runs, or restructuring the pipeline to handle the training and transformation in a way that doesn\\'t require both steps to be dependent on each other.\\\\\\\\n\\\\\\\\nI should make sure my answers are clear and directly address each potential question. I\\'ll need to explain the problem in a way that\\'s easy to understand, perhaps by breaking it down into steps or components.\\\\\\\\n</think>\\\\\\\\n\\\\\\\\n**Q1: What is the main challenge in integrating the project into an MLOps pipeline?**\\\\\\\\n\\\\\\\\n**A1:** The main challenge is a circular dependency between the model training and data transformation steps. The AutoEncoder must be trained before it can be used for dimensionality reduction in the data transformation step. However, the current pipeline structure places data transformation after model training, creating a loop where each step depends on the other.\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n**Q2: How does the AutoEncoder contribute to dimensionality reduction in this project?**\\\\\\\\n\\\\\\\\n**A2:** The AutoEncoder is trained to learn a compressed representation of the high-dimensional hyperpectral image (HSI) data. Once trained, it is used in the data transformation step to reduce the dimensionality of the data, making it more manageable for the subsequent classification model.\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n**Q3: What steps are involved in the project\\'s workflow?**\\\\\\\\n\\\\\\\\n**A3:** The workflow consists of the following steps:\\\\\\\\n\\\\\\\\n1. **Data Ingestion:** Loading the hyperpectral image dataset.\\\\\\\\n2. **Model Training (AutoEncoder):** Training an AutoEncoder for dimensionality reduction.\\\\\\\\n3. **Data Transformation:** Applying the trained AutoEncoder to transform the data.\\\\\\\\n4. **Model Training (AutoEncoder + Classifier):** Fine-tuning the model with both the AutoEncoder and classifier using the transformed data.\\\\\\\\n5. **Prediction:** Using the trained model for classifying new hyperpectral images.\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n**Q4: How is the project structured?**\\\\\\\\n\\\\\\\\n**A4:** The project is structured with the following key directories and files:\\\\\\\\n\\\\\\\\n- **config/**: Contains configuration files, such as `config.yaml`, which specifies paths and parameters.\\\\\\\\n- **DATA/**: Stores the dataset used for training and inference.\\\\\\\\n- **Notebooks/**: Holds Jupyter Notebooks for research and experimentation.\\\\\\\\n- **src/**: The main source code directory, which includes:\\\\\\\\n  - **components/**: Handles core pipeline components like data ingestion, transformation, and model training.\\\\\\\\n  - **models/**: Contains model architectures, such as the CNN and AutoEncoder + Classifier.\\\\\\\\n  - **pipeline/**: Includes scripts for the full training pipeline (`training_pipeline.py`) and inference (`predict_pipeline.py`).\\\\\\\\n  - **utils.py**: Provides utility functions for logging, preprocessing, etc.\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n**Q5: Why is the circular dependency a problem in the pipeline?**\\\\\\\\n\\\\\\\\n**A5:** The circular dependency arises because the model_trainer step depends on the data being transformed, which itself requires the AutoEncoder to be trained. However, in the pipeline, the data_transformation step comes after model_trainer, creating a loop where each step requires the other to have already been completed. This makes it difficult to execute the pipeline in a linear, sequential manner.\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n**Q6: What are the possible solutions to break the circular dependency?**\\\\\\\\n\\\\\\\\n**A6:** Possible solutions to resolve the circular dependency include:\\\\\\\\n\\\\\\\\n1. **Separate Training Phase for AutoEncoder:**\\\\\\\\n   - Train the AutoEncoder independently before the main pipeline runs. This way, the transformed data can be used in subsequent steps without requiring the model_trainer to run first.\\\\\\\\n\\\\\\\\n2. **Refactor the Pipeline:**\\\\\\\\n   - Restructure the pipeline to handle the training of the AutoEncoder and the transformation step in a way that avoids the dependency loop. This might involve separating the AutoEncoder training from the main model training.\\\\\\\\n\\\\\\\\n3. **Use Pre-trained Models:**\\\\\\\\n   - If the AutoEncoder can be pre-trained on a similar dataset, it can be loaded and used for data transformation without needing to train it within the pipeline.\\\\\\\\n\\\\\\\\n4. **Modularize Components:**\\\\\\\\n   - Break down the pipeline into more modular components, allowing the AutoEncoder training and data transformation to be executed as separate tasks that can be orchestrated independently.\\\\\\\\n\\\\\\\\nBy addressing this circular dependency, the pipeline can be made more efficient and suitable for integration into an MLOps framework.\\\\\"\"\\n      ]\\n     },\\n     \"execution_count\": 12,\\n     \"metadata\": {},\\n     \"output_type\": \"execute_result\"\\n    }\\n   ],\\n   \"source\": [\\n    \"# Now we will combine our document chain and retriever chain so that llm can have context about documents\\\\n\",\\n    \"prompt= ChatPromptTemplate.from_template(\\\\n\",\\n    \"    \\\\\"\\\\\"\\\\\"\\\\n\",\\n    \"    Answer the following questions based on the provided context:\\\\n\",\\n    \"    <context>\\\\n\",\\n    \"    {context}\\\\n\",\\n    \"    </context>\\\\n\",\\n    \"    \\\\\"\\\\\"\\\\\"\\\\n\",\\n    \")\\\\n\",\\n    \"\\\\n\",\\n    \"document_chain=create_stuff_documents_chain(llm_engine,prompt)\\\\n\",\\n    \"\\\\n\",\\n    \"# create the retriever chain\\\\n\",\\n    \"retrieval_chain=create_retrieval_chain(retriever, document_chain)\\\\n\",\\n    \"\\\\n\",\\n    \"# Get the response\\\\n\",\\n    \"response=retrieval_chain.invoke({\\\\\"input\\\\\":\\\\\"what is the major problem faced in the project\\\\\"})\\\\n\",\\n    \"response[\\\\\"answer\\\\\"]\"\\n   ]\\n  },\\n  {\\n   \"cell_type\": \"code\",\\n   \"execution_count\": null,\\n   \"metadata\": {},\\n   \"outputs\": [],\\n   \"source\": []\\n  }\\n ],\\n \"metadata\": {\\n  \"kernelspec\": {\\n   \"display_name\": \"novaenv\",\\n   \"language\": \"python\",\\n   \"name\": \"python3\"\\n  },\\n  \"language_info\": {\\n   \"codemirror_mode\": {\\n    \"name\": \"ipython\",\\n    \"version\": 3\\n   },\\n   \"file_extension\": \".py\",\\n   \"mimetype\": \"text/x-python\",\\n   \"name\": \"python\",\\n   \"nbconvert_exporter\": \"python\",\\n   \"pygments_lexer\": \"ipython3\",\\n   \"version\": \"3.10.16\"\\n  }\\n },\\n \"nbformat\": 4,\\n \"nbformat_minor\": 2\\n}\\n'}, {'path': 'README.md', 'content': '# üåå NOVA - Intelligent Multi-Document AI Assistant\\n\\n[![Streamlit](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://nova-ai-v1.streamlit.app/)\\n![LLM Powered](https://img.shields.io/badge/LLM-Powered-blueviolet)\\n![Multi-Document](https://img.shields.io/badge/Multi-Document-Analysis-success)\\n![Real-Time Search](https://img.shields.io/badge/Real--Time-Web%20Search-important)\\n\\n**NOVA** is an advanced AI assistant with built-in multi-document intelligence and real-time web search capabilities. Designed for professionals, researchers, and curious minds, NOVA combines state-of-the-art language models with powerful document analysis tools.\\n\\n\\n**Access the Deployed version of NOVA Here**: https://nova-ai-v1.streamlit.app\\n\\n## ‚ú® Features\\n\\n- **Multi-Document Intelligence**\\n  - üìÑ Support for 15+ file types (PDF, DOCX, XLSX, PPT, CSV, JSON, etc.)\\n  - üîç Semantic search across uploaded documents\\n  - üìä Structured data analysis (Excel, CSV, JSON)\\n- **Smart Web Integration**\\n  - üåê Real-time web search powered by Tavily\\n  - ‚ö° Automatic search triggering based on query context\\n- **Advanced NLP Capabilities**\\n  - üí¨ Natural conversation interface\\n  - ü§ñ Groq-powered ultra-fast LLM responses\\n  - üîß Customizable system prompts and templates\\n- **Enterprise-Ready Features**\\n  - üß† In-memory vector store for document embeddings\\n  - ‚öôÔ∏è Chunk-based document processing\\n  - üé® Streamlit-based professional UI\\n\\n## üöÄ Quick Start\\n\\n### Prerequisites\\n- Python 3.9+\\n- API keys for:\\n  - [Groq](https://console.groq.com/)\\n  - [Tavily](https://tavily.com/)\\n  - [Cohere](https://dashboard.cohere.com/) \\n  - [HuggingFace](https://huggingface.co/settings/tokens) \\n\\n### Installation\\n```bash\\n# Clone repository\\ngit clone https://github.com/yourusername/nova-ai-assistant.git\\ncd nova-ai-assistant\\n\\n# Create virtual environment\\npython -m venv venv\\nsource venv/bin/activate  # Linux/Mac\\n# venv\\\\Scripts\\\\activate  # Windows\\n\\n# Install dependencies\\npip install -r requirements.txt\\n\\n# Set up environment variables\\ncp .env.example .env\\n# Add your API keys to .env\\n\\n# ‚öôÔ∏è Configuration\\nEdit the .env file with your API credentials:\\nGROQ_API_KEY=your_groq_key\\nTAVILY_API_KEY=your_tavily_key\\nCOHERE_API_KEY=your_cohere_key  \\nHUGGINGFACE_API_KEY=your_hf_key \\n\\n# üíª Usage\\nstreamlit run app.py\\n```\\n### Basic Workflow:\\n\\n1. Upload documents through the web interface\\n\\n2. Ask questions about content or request analysis\\n\\n3. NOVA automatically determines when to use:\\n\\n- Document content\\n\\n- Web search results\\n\\n- Internal knowledge base\\n\\n## Example Queries:\\n\\n- \"Summarize the key points from my PDF\"\\n\\n- \"Compare Q2 sales figures from the Excel sheet\"\\n\\n- \"What\\'s the latest news about AI advancements?\"\\n\\n- \"Create a bullet list from Section 3 of the contract\"\\n\\n\\n### Project Structure\\n```bash\\nnova-ai-assistant/\\n‚îú‚îÄ‚îÄ app.py                 # Main Streamlit application\\n‚îú‚îÄ‚îÄ config.py              # Configuration settings\\n‚îú‚îÄ‚îÄ models/\\n‚îÇ   ‚îî‚îÄ‚îÄ llm.py             # LLM initialization and tools\\n‚îú‚îÄ‚îÄ prompts/\\n‚îÇ   ‚îî‚îÄ‚îÄ templates.py       # System prompt templates\\n‚îú‚îÄ‚îÄ utils/\\n‚îÇ   ‚îú‚îÄ‚îÄ document_utils.py  # Document processing\\n‚îÇ   ‚îú‚îÄ‚îÄ search_utils.py    # Web search functions\\n‚îÇ   ‚îî‚îÄ‚îÄ ui_utils.py        # Streamlit UI components\\n‚îî‚îÄ‚îÄ requirements.txt       # Dependencies\\n```\\n\\n### Key Components\\n* Document Processing\\n    - Automatic file type detection\\n    - Chunk-based text splitting\\n    - In-memory vector store with Cohere/HuggingFace embeddings\\n\\n* AI Pipeline\\n    - Dynamic prompt engineering\\n    - Automatic tool selection (web/doc search)\\n    - Thought process visualization\\n\\n* UI Features\\n    - Dark/light mode support\\n    - Document preview pane\\n    - Interactive chat history\\n    - File-type specific visual indicators\\n\\n\\n### ü§ù Contributing\\nWe welcome contributions! Please follow these steps:\\n1. Fork the repository\\n    \\n2. Create your feature branch (git checkout -b feature/AmazingFeature)\\n    \\n3. Commit your changes (git commit -m \\'Add some AmazingFeature\\')\\n   \\n4. Push to the branch (git push origin feature/AmazingFeature)\\n   \\n5. Open a Pull Request\\n\\n\\n### üìú License\\nDistributed under the MIT License. See LICENSE for more information.\\n\\n### üåü Acknowledgments\\n- Groq for ultra-fast LLM inference\\n- Tavily for real-time web search API\\n- LangChain team for document processing tools\\n- Streamlit for interactive UI framework\\n- Developed by [Lalan Kumar](https://github.com/kumar8074)\\n'}, {'path': 'app.py', 'content': 'import streamlit as st\\nimport re\\nimport os\\nimport datetime\\nfrom io import BytesIO\\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\\n\\n# Import from local modules\\nfrom config import CURRENT_DATE\\nfrom utils.ui_utils import apply_custom_styling, initialize_session_state\\nfrom utils.document_utils import process_document_file, query_documents, needs_document_search, get_supported_file_types\\nfrom utils.search_utils import perform_web_search, needs_web_search\\nfrom models.llm import initialize_llm, initialize_embeddings, initialize_vector_store, build_prompt_chain\\nfrom prompts.templates import SYSTEM_TEMPLATE, SEARCH_INSTRUCTION_TEMPLATE, DOCUMENT_INSTRUCTION_TEMPLATE\\n\\n# Apply custom styling\\napply_custom_styling()\\n\\n# Initialize session state\\ninitialize_session_state()\\n\\n# Set up the page\\nst.title(\"NOVA\")\\nst.caption(\"Your AI Assistant with Multi-Document Intelligence\")\\n\\n# Initialize LLM and embeddings\\nllm_engine = initialize_llm()\\nembedding_model = initialize_embeddings()\\n\\n# Initialize vector store if not already done\\nif st.session_state.vector_store is None:\\n    st.session_state.vector_store = initialize_vector_store(embedding_model)\\n\\n# Create main chat container\\nchat_container = st.container()\\n\\n# Display previous messages\\nwith chat_container:\\n    for message in st.session_state.message_log:\\n        with st.chat_message(message[\"role\"]):\\n            content = message[\"content\"]\\n            think_matches = re.findall(r\\'<think>(.*?)</think>\\', content, flags=re.DOTALL)\\n            content_without_think = re.sub(r\\'<think>.*?</think>\\', \\'\\', content, flags=re.DOTALL)\\n\\n            st.markdown(content_without_think)\\n\\n            for think_text in think_matches:\\n                with st.expander(\"üí≠ Thought Process\"):\\n                    st.markdown(think_text)\\n    \\n    # Show processing indicator only when processing\\n    if st.session_state.processing:\\n        with st.chat_message(\"ai\"):\\n            st.write(\"Processing...\")\\n\\n# Display uploaded documents if any\\nif st.session_state.uploaded_files:\\n    with st.expander(\"üìö Uploaded Documents\"):\\n        for file_name, file_info in st.session_state.uploaded_file_info.items():\\n            file_type = file_info.get(\"file_type\", \"Unknown\")\\n            file_size = file_info.get(\"file_size_kb\", 0)\\n            st.write(f\"- {file_name} ({file_type}, {file_size} KB)\")\\n\\n# Create a container for the upload button and chat input\\ninput_container = st.container()\\n\\nwith input_container:\\n    # Create two columns for the chat input and upload button\\n    col1, col2 = st.columns([10, 2])\\n    \\n    with col2:\\n        # Simple upload button\\n        upload_button = st.button(\"üìÑ +\", key=\"upload_button\")\\n        if upload_button:\\n            st.session_state.show_uploader = not st.session_state.show_uploader\\n    \\n    with col1:\\n        # Chat input\\n        user_query = st.chat_input(\"Ask NOVA...\")\\n\\n# Get supported file types for the uploader\\nsupported_file_types = get_supported_file_types()\\n\\n# Show the file uploader when toggled\\nif st.session_state.show_uploader:\\n    uploaded_file = st.file_uploader(\\n        \"Upload Document\",\\n        type=supported_file_types,\\n        key=\"file_uploader\"\\n    )\\n    \\n    # Process uploaded file\\n    if uploaded_file and (st.session_state.last_uploaded_file != uploaded_file.name):\\n        with st.spinner(\"Processing document...\"):\\n            try:\\n                # Process the document\\n                processing_results = process_document_file(uploaded_file, st.session_state.vector_store)\\n                \\n                # Extract results\\n                num_chunks = processing_results.get(\"chunks\", 0)\\n                file_type = processing_results.get(\"file_type\", \"UNKNOWN\")\\n                file_size = processing_results.get(\"file_size_kb\", 0)\\n                \\n                # Store file info for display\\n                st.session_state.uploaded_file_info[uploaded_file.name] = processing_results\\n                \\n                if num_chunks > 0:\\n                    # Update state\\n                    st.session_state.has_documents = True\\n                    if uploaded_file.name not in st.session_state.uploaded_files:\\n                        st.session_state.uploaded_files.append(uploaded_file.name)\\n                    \\n                    # Add system message about the upload\\n                    upload_message = f\"üìÑ Document \\'{uploaded_file.name}\\' successfully uploaded and processed ({num_chunks} chunks, {file_size} KB, {file_type}). You can now ask questions about this document.\"\\n                    st.session_state.message_log.append({\"role\": \"ai\", \"content\": upload_message})\\n                    \\n                    # Update last uploaded file to prevent duplicate messages\\n                    st.session_state.last_uploaded_file = uploaded_file.name\\n                    \\n                    # Hide the uploader after successful upload\\n                    st.session_state.show_uploader = False\\n                    \\n                    # Rerun to update UI\\n                    st.rerun()\\n                else:\\n                    st.error(f\"Failed to process document \\'{uploaded_file.name}\\'. No content could be extracted.\")\\n            except Exception as e:\\n                st.error(f\"Failed to process document \\'{uploaded_file.name}\\': {str(e)}\")\\n\\n# Handle user input\\nif user_query:\\n    # Add user message to chat history immediately\\n    st.session_state.message_log.append({\"role\": \"user\", \"content\": user_query})\\n    st.session_state.processing = True\\n    st.rerun()\\n\\n# Continue processing if in processing state\\nif st.session_state.processing:\\n    with st.spinner(\"\"):\\n        messages = build_prompt_chain(st.session_state.message_log, SYSTEM_TEMPLATE)\\n        \\n        # Get the last user query\\n        last_user_query = st.session_state.message_log[-1][\"content\"]\\n        \\n        # Determine if web search or document search is needed\\n        should_search_web = needs_web_search(last_user_query)\\n        should_search_docs = needs_document_search(last_user_query)\\n        \\n        # Flag to track if we\\'ve already handled the query\\n        query_handled = False\\n        \\n        # Always try document search first if we have documents\\n        if st.session_state.has_documents:\\n            # Query documents\\n            document_results = query_documents(last_user_query)\\n            \\n            if \"No documents have been uploaded yet\" not in document_results and \"Error\" not in document_results:\\n                # Format document instruction with results\\n                doc_instruction = DOCUMENT_INSTRUCTION_TEMPLATE.format(document_results=document_results)\\n                \\n                # Add the document results as context\\n                doc_context_message = SystemMessage(content=doc_instruction)\\n                \\n                # Create new messages list with the document context\\n                doc_messages = [\\n                    SystemMessage(content=SYSTEM_TEMPLATE),\\n                    doc_context_message,\\n                    HumanMessage(content=f\"{last_user_query}\")\\n                ]\\n                \\n                # Get response from LLM with document results\\n                ai_response = llm_engine.invoke(doc_messages).content\\n                query_handled = True\\n                \\n                # Add a thought process about document search\\n                if \"<think>\" not in ai_response:\\n                    ai_response += f\"\\\\n\\\\n<think>Document search was performed and used to generate this response.</think>\"\\n        \\n        # Handle web search if needed and not already handled\\n        if should_search_web and not query_handled:\\n            # Perform web search\\n            search_results = perform_web_search(last_user_query)\\n            \\n            # Format search instruction with results\\n            search_instruction = SEARCH_INSTRUCTION_TEMPLATE.format(search_results=search_results)\\n            \\n            # Add the search results as context\\n            web_context_message = SystemMessage(content=search_instruction)\\n            \\n            # Create new messages list with the search context\\n            search_messages = [\\n                SystemMessage(content=SYSTEM_TEMPLATE),\\n                web_context_message,\\n                HumanMessage(content=f\"{last_user_query}\")\\n            ]\\n            \\n            # Get response from LLM with search results\\n            ai_response = llm_engine.invoke(search_messages).content\\n            query_handled = True\\n            \\n            # Add a thought process about web search\\n            if \"<think>\" not in ai_response:\\n                ai_response += f\"\\\\n\\\\n<think>Web search was performed and used to generate this response.</think>\"\\n        \\n        # If neither search was used or they didn\\'t provide useful results\\n        if not query_handled:\\n            # Add the current user query to the messages\\n            messages.append(HumanMessage(content=last_user_query))\\n            \\n            # Use LLM directly\\n            ai_response = llm_engine.invoke(messages).content\\n            \\n            # Add a thought process about using base knowledge\\n            if \"<think>\" not in ai_response:\\n                ai_response += f\"\\\\n\\\\n<think>No external search was performed. Response generated from base knowledge.</think>\"\\n\\n    # Add AI response to chat history\\n    st.session_state.message_log.append({\"role\": \"ai\", \"content\": ai_response})\\n    \\n    # Turn off processing state\\n    st.session_state.processing = False\\n    \\n    # Rerun to update the UI\\n    st.rerun()'}, {'path': 'config.py', 'content': 'import os\\nimport datetime\\nfrom dotenv import load_dotenv\\n\\n# Load environment variables\\nload_dotenv()\\n\\n# API keys\\nGROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\\nTAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\\nCOHERE_API_KEY = os.getenv(\"COHERE_API_KEY\")\\nHUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\\nGEMINI_API_KEY=os.getenv(\"GOOGLE_API_KEY\")\\n\\n# Model settings\\nLLM_MODEL = \"gemini-2.0-flash\"\\n\\n\\n# Get current date\\nCURRENT_DATE = datetime.datetime.now().strftime(\"%Y-%m-%d\")\\n\\n# Document processing settings\\nCHUNK_SIZE = 1000\\nCHUNK_OVERLAP = 200\\n'}, {'path': 'models/llm.py', 'content': 'import os\\nfrom langchain_groq import ChatGroq\\nfrom langchain_cohere import CohereEmbeddings\\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\\nfrom langchain_core.vectorstores import InMemoryVectorStore\\nfrom langchain.tools import Tool\\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\\nfrom config import GROQ_API_KEY, LLM_MODEL, COHERE_API_KEY, HUGGINGFACE_API_KEY, GEMINI_API_KEY\\nfrom utils.document_utils import query_documents\\nfrom utils.search_utils import perform_web_search\\nfrom langchain_google_genai import ChatGoogleGenerativeAI\\n\\nos.environ[\"COHERE_API_KEY\"] = COHERE_API_KEY\\nos.environ[\"HUGGING_FACE_API_KEY\"] = HUGGINGFACE_API_KEY\\nos.environ[\"GEMINI_API_KEY\"]=GEMINI_API_KEY\\n\\ndef initialize_llm():\\n    \"\"\"Initialize the LLM engine\"\"\"\\n    return ChatGoogleGenerativeAI(model=LLM_MODEL)\\n\\ndef initialize_embeddings():\\n    \"\"\"Initialize the embedding model\"\"\"\\n    try:\\n        return CohereEmbeddings(model=\"embed-english-v3.0\") \\n    except Exception:\\n        return HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\\n\\ndef initialize_vector_store(embedding_model):\\n    \"\"\"Initialize the vector store\"\"\"\\n    return InMemoryVectorStore(embedding=embedding_model)\\n\\ndef get_tools():\\n    \"\"\"Get the available tools for the assistant\"\"\"\\n    # Define Web Search and Document Query as Tools\\n    web_search_tool = Tool(\\n        name=\"Web Search\",\\n        func=perform_web_search,\\n        description=\"Use this tool to fetch the latest information from the web. Input a search query.\"\\n    )\\n\\n    document_query_tool = Tool(\\n        name=\"Document Query\",\\n        func=query_documents,\\n        description=\"Use this tool to search through uploaded documents. Input a search query.\"\\n    )\\n    \\n    return [web_search_tool, document_query_tool]\\n\\ndef build_prompt_chain(message_log, system_template):\\n    \"\"\"Build the prompt chain from message history\"\"\"\\n    # Start with just the system message\\n    messages = [SystemMessage(content=system_template)]\\n    \\n    # Add the conversation history\\n    for msg in message_log:\\n        if msg[\"role\"] == \"user\":\\n            messages.append(HumanMessage(content=msg[\"content\"]))\\n        elif msg[\"role\"] == \"ai\":\\n            messages.append(AIMessage(content=msg[\"content\"]))\\n    \\n    return messages\\n'}, {'path': 'nova.py', 'content': '######### GROUND TRUTH FULL CODE BASE FOR NOVA #########\\n\\nimport os\\nimport streamlit as st\\nimport re\\nimport datetime\\nfrom dotenv import load_dotenv\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_groq import ChatGroq\\nfrom langchain.tools import Tool\\nfrom langchain_core.runnables import RunnableBranch\\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\\nfrom tavily import TavilyClient\\nfrom io import BytesIO\\n\\n# Document processing imports\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_core.vectorstores import InMemoryVectorStore\\nfrom langchain_ollama import OllamaEmbeddings\\n\\n# Load environment variables\\nload_dotenv()\\n\\n# Get API keys\\ngroq_api_key = os.getenv(\"GROQ_API_KEY\")\\ntavily_api_key = os.getenv(\"TAVILY_API_KEY\")\\n\\n# Initialize Tavily client\\ntavily_client = TavilyClient(api_key=tavily_api_key)\\n\\n# Initialize embedding model\\nEMBEDDING_MODEL = OllamaEmbeddings(model=\"deepseek-r1:1.5b\")\\n\\n# Apply custom styling\\nst.markdown(\"\"\"\\n    <style>\\n    .stApp {\\n        background-color: #0E1117;\\n        color: #FFFFFF;\\n    }\\n    \\n    /* Chat Input Styling */\\n    .stChatInput input {\\n        background-color: #1E1E1E !important;\\n        color: #FFFFFF !important;\\n        border: 1px solid #3A3A3A !important;\\n    }\\n    \\n    /* User Message Styling */\\n    .stChatMessage[data-testid=\"stChatMessage\"]:nth-child(odd) {\\n        background-color: #1E1E1E !important;\\n        border: 1px solid #3A3A3A !important;\\n        color: #E0E0E0 !important;\\n        border-radius: 10px;\\n        padding: 15px;\\n        margin: 10px 0;\\n    }\\n    \\n    /* Assistant Message Styling */\\n    .stChatMessage[data-testid=\"stChatMessage\"]:nth-child(even) {\\n        background-color: #2A2A2A !important;\\n        border: 1px solid #404040 !important;\\n        color: #F0F0F0 !important;\\n        border-radius: 10px;\\n        padding: 15px;\\n        margin: 10px 0;\\n    }\\n    \\n    /* Avatar Styling */\\n    .stChatMessage .avatar {\\n        background-color: #00FFAA !important;\\n        color: #000000 !important;\\n    }\\n    \\n    /* Text Color Fix */\\n    .stChatMessage p, .stChatMessage div {\\n        color: #FFFFFF !important;\\n    }\\n    \\n    .stFileUploader {\\n        background-color: #1E1E1E;\\n        border: 1px solid #3A3A3A;\\n        border-radius: 5px;\\n        padding: 15px;\\n    }\\n    \\n    h1, h2, h3 {\\n        color: #00FFAA !important;\\n    }\\n    \\n    /* Style for upload button */\\n    .stButton button {\\n        background-color: #1E1E1E !important;\\n        color: #00FFAA !important;\\n        border: 1px solid #3A3A3A !important;\\n        border-radius: 5px;\\n        padding: 5px 10px;\\n        font-weight: bold;\\n        font-size: 16px;\\n    }\\n    \\n    .stButton button:hover {\\n        background-color: #2A2A2A !important;\\n        border: 1px solid #00FFAA !important;\\n    }\\n    </style>\\n    \"\"\", unsafe_allow_html=True)\\n\\nst.title(\"NOVA\")\\nst.caption(\"Your AI Assistant with Document Intelligence\")\\n\\n# Initialize AI model (DeepSeek on Groq)\\nllm_engine = ChatGroq(model=\"Deepseek-R1-Distill-Qwen-32b\", groq_api_key=groq_api_key)\\n\\n# Initialize session state\\nif \"vector_store\" not in st.session_state:\\n    st.session_state.vector_store = InMemoryVectorStore(embedding=EMBEDDING_MODEL)\\n\\nif \"document_contents\" not in st.session_state:\\n    st.session_state.document_contents = {}\\n\\n# Document processing functions\\ndef process_pdf_file(uploaded_file):\\n    # Create a temporary file-like object\\n    pdf_file = BytesIO(uploaded_file.getvalue())\\n    \\n    # Save to a temporary file that PDFPlumberLoader can use\\n    temp_path = f\"temp_{uploaded_file.name}\"\\n    with open(temp_path, \"wb\") as f:\\n        f.write(pdf_file.getvalue())\\n        \\n    # Use the regular PDFPlumberLoader\\n    loader = PDFPlumberLoader(temp_path)\\n    try:\\n        raw_docs = loader.load()\\n        \\n        # Store the raw document content for direct access\\n        full_text = \"\\\\n\\\\n\".join([doc.page_content for doc in raw_docs])\\n        st.session_state.document_contents[uploaded_file.name] = full_text\\n        \\n        # Chunk documents\\n        text_processor = RecursiveCharacterTextSplitter(\\n            chunk_size=1000,\\n            chunk_overlap=200,\\n            add_start_index=True\\n        )\\n        document_chunks = text_processor.split_documents(raw_docs)\\n        \\n        # Add metadata to track source document\\n        for chunk in document_chunks:\\n            if \"source\" not in chunk.metadata:\\n                chunk.metadata[\"source\"] = uploaded_file.name\\n        \\n        # Add to vector store\\n        st.session_state.vector_store.add_documents(document_chunks)\\n        \\n        # Clean up the temporary file\\n        os.remove(temp_path)\\n        \\n        return len(document_chunks)\\n    except Exception as e:\\n        if os.path.exists(temp_path):\\n            os.remove(temp_path)\\n        st.error(f\"Error processing PDF: {str(e)}\")\\n        return 0\\n\\n# Define document RAG function with improved error handling and debugging\\ndef query_documents(query: str) -> str:\\n    try:\\n        # Debug information\\n        doc_count = 0\\n        try:\\n            # This is a safer way to check document count that won\\'t crash if structure changes\\n            if hasattr(st.session_state.vector_store, \"_collection\"):\\n                doc_count = st.session_state.vector_store._collection.count()\\n            else:\\n                # Alternative method if _collection doesn\\'t exist\\n                doc_count = len(st.session_state.document_contents)\\n        except:\\n            doc_count = len(st.session_state.document_contents)\\n        \\n        # Check if there are documents in the vector store\\n        if doc_count == 0:\\n            return \"No documents have been uploaded yet. Please upload a document first to enable document queries.\"\\n        \\n        # Find related documents\\n        try:\\n            relevant_docs = st.session_state.vector_store.similarity_search(query, k=4)\\n        except Exception as e:\\n            # Fallback to direct document search if vector search fails\\n            if len(st.session_state.document_contents) > 0:\\n                fallback_results = []\\n                for doc_name, content in st.session_state.document_contents.items():\\n                    fallback_results.append(f\"Document: {doc_name}\\\\nContent: {content[:1000]}...\")\\n                \\n                context_text = \"\\\\n\\\\n\".join(fallback_results)\\n                \\n                # Log the issue but continue with fallback\\n                print(f\"Vector search failed, using fallback: {str(e)}\")\\n                \\n                return f\"Vector search failed, using direct document content.\\\\n\\\\n{context_text}\"\\n            else:\\n                return f\"Error searching documents: {str(e)}\"\\n        \\n        if not relevant_docs:\\n            # Fallback to direct document content search\\n            if len(st.session_state.document_contents) > 0:\\n                matching_docs = []\\n                for doc_name, content in st.session_state.document_contents.items():\\n                    # Simple keyword matching fallback\\n                    query_keywords = query.lower().split()\\n                    if any(keyword in content.lower() for keyword in query_keywords):\\n                        matching_docs.append(f\"Document: {doc_name}\\\\nContent: {content[:1000]}...\")\\n                \\n                if matching_docs:\\n                    context_text = \"\\\\n\\\\n\".join(matching_docs)\\n                    return context_text\\n                else:\\n                    return \"No relevant information found in the uploaded documents based on direct search.\"\\n            else:\\n                return \"No relevant information found in the uploaded documents.\"\\n        \\n        # Create context from documents with source tracking\\n        doc_contexts = []\\n        for i, doc in enumerate(relevant_docs):\\n            source = doc.metadata.get(\"source\", f\"Document {i+1}\")\\n            doc_contexts.append(f\"Document: {source}\\\\nContent: {doc.page_content}\")\\n        \\n        context_text = \"\\\\n\\\\n\".join(doc_contexts)\\n        \\n        # Document QA prompt\\n        doc_qa_prompt = \"\"\"\\n        You are an expert research assistant. Use ONLY the provided document context to answer the query.\\n        If the answer is not in the provided context, state that you don\\'t have the information.\\n        Be clear, factual, and provide specific references to the document parts you\\'re using.\\n        \\n        Query: {query}\\n        Document Context: {context}\\n        \\n        Answer:\\n        \"\"\"\\n        \\n        prompt = ChatPromptTemplate.from_template(doc_qa_prompt)\\n        response_chain = prompt | llm_engine\\n        \\n        response = response_chain.invoke({\"query\": query, \"context\": context_text})\\n        \\n        # Add source information\\n        result = response.content\\n        if not result.strip().lower().startswith(\"i don\\'t have\"):\\n            result += f\"\\\\n\\\\n<think>Documents searched: {len(relevant_docs)} chunks from {\\', \\'.join(set([doc.metadata.get(\\'source\\', \\'Unknown\\') for doc in relevant_docs]))}</think>\"\\n        \\n        return result\\n    except Exception as e:\\n        # Be more specific about the error and include debugging information\\n        error_message = f\"Error querying documents: {str(e)}\"\\n        print(error_message)  # For server logs\\n        \\n        # Include information about the document store state\\n        doc_info = \"No document information available\"\\n        if hasattr(st.session_state, \"document_contents\"):\\n            doc_names = list(st.session_state.document_contents.keys())\\n            doc_info = f\"Available documents: {\\', \\'.join(doc_names) if doc_names else \\'None\\'}\"\\n        \\n        return f\"{error_message}\\\\n{doc_info}\"\\n\\n# Function to perform internet search\\ndef perform_web_search(query: str) -> str:\\n    try:\\n        response = tavily_client.search(query, search_depth=\"advanced\", max_results=5)\\n        if response and \"results\" in response and len(response[\"results\"]) > 0:\\n            formatted_results = []\\n            for i, res in enumerate(response[\"results\"], 1):\\n                title = res.get(\\'title\\', \\'No title\\')\\n                url = res.get(\\'url\\', \\'#\\')\\n                content = res.get(\\'content\\', \\'No description available.\\')\\n                \\n                # Format the result with source number for easier reference\\n                formatted_results.append(f\"Source {i}: {title}\\\\nURL: {url}\\\\nContent: {content}\\\\n\")\\n            \\n            return \"\\\\n\".join(formatted_results)\\n        return \"No relevant search results found.\"\\n    except Exception as e:\\n        return f\"Error performing web search: {str(e)}\"\\n\\n# Define Web Search and Document Query as Tools\\nweb_search_tool = Tool(\\n    name=\"Web Search\",\\n    func=perform_web_search,\\n    description=\"Use this tool to fetch the latest information from the web. Input a search query.\"\\n)\\n\\ndocument_query_tool = Tool(\\n    name=\"Document Query\",\\n    func=query_documents,\\n    description=\"Use this tool to search through uploaded documents. Input a search query.\"\\n)\\n\\n# Get current date\\ncurrent_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\\n\\n# Updated system message template with emotion-related content removed\\nsystem_template = f\"\"\"\\nYou are NOVA, an expert AI assistant with document analysis and web search capabilities, created by KUMAR.\\n\\nToday\\'s date is {current_date}.\\n\\nCore capabilities:\\n1. Concise and accurate responses tailored to user needs\\n2. Real-time information retrieval via web search when necessary\\n3. Document analysis and question answering for uploaded PDF documents\\n\\nWhen responding:\\n- If the user asks about current events, real-time data, or information you\\'re uncertain about, use the web search results\\n- If the user asks about uploaded documents, use the document query results\\n- ALWAYS use web search or document query results when provided to give accurate information\\n- Summarize information from search or document results in your own words\\n- Cite sources by referring to \"Source 1\", \"Source 2\", etc. or \"Document 1\", \"Document 2\", etc and provide their relevant links.\\n- NEVER claim you can\\'t access links, real-time data, or the internet\\n- When asked about dates, times, or current events, ALWAYS reference today\\'s date: {current_date}\\n- If search or document results are provided BUT they\\'re not relevant to the query, rely on your training instead\\n- If no search or document results are available, respond based on your training\\n\\nRemember: For most conversational queries, you don\\'t need web search or document queries. Use them only when necessary.\\n\\nIMPORTANT DOCUMENT HANDLING INSTRUCTIONS:\\n- When users upload documents, you CAN access their content through the Document Query tool\\n- NEVER say you cannot access document content - you have direct access to all uploaded documents\\n- When answering document questions, cite specific parts of the document\\n- If a document doesn\\'t contain relevant information, clearly state that instead of saying you can\\'t access it\\n- If the document doesn\\'t contain relevant information, use web search to answer the query, AND clearly state that you have used the websearch\\n\"\"\"\\n\\n# Enhanced search system message\\nsearch_instruction_template = \"\"\"\\nIMPORTANT: You have access to recent web search results about the user\\'s query.\\n\\nThese search results may contain current information that you can use to answer the user\\'s question.\\n\\nPlease follow these guidelines:\\n1. Evaluate if the search results are RELEVANT to the user\\'s query\\n2. If relevant, synthesize information from the search results for an accurate, up-to-date answer\\n3. If NOT relevant, DO NOT use them and instead rely on your training\\n4. When using search results, cite your sources by referring to \"Source 1\", \"Source 2\", etc and provide their relevant links.\\n5. If the search results contain conflicting information, acknowledge this and present multiple perspectives\\n6. Present information in a clear, concise manner\\n7. NEVER claim you can\\'t access real-time data or the internet\\n\\nHere are the search results:\\n\\n{search_results}\\n\"\"\"\\n\\n# Document query system message\\ndocument_instruction_template = \"\"\"\\nIMPORTANT: You have access to information from documents that the user has uploaded. The following content has been extracted from these documents.\\n\\nPlease follow these guidelines:\\n1. You CAN and SHOULD use this document information to answer the user\\'s query\\n2. NEVER say you cannot access document content - you have direct access shown below\\n3. If the information below doesn\\'t answer the query, state \"The uploaded documents don\\'t contain information about [specific topic]\" \\n4. If the uploaded documents does not contain information about [specific topic], use websearch to answer the query and provide relevant souce links\\n5. When using document information, cite your sources by referring to specific document names\\n6. Be specific about what parts of the documents you\\'re using\\n7. Present information in a clear, concise manner\\n8. Do not make up information that isn\\'t in the documents\\n\\nHere is the document information:\\n\\n{document_results}\\n\"\"\"\\n\\n# Initialize message log in session state\\nif \"message_log\" not in st.session_state:\\n    st.session_state.message_log = [\\n        {\"role\": \"ai\", \"content\": \"Hi! I\\'m NOVA, your AI assistant with document intelligence. What can I do for you today? You can chat with me or upload PDF documents for analysis.\"}\\n    ]\\n\\n# Flag for processing state\\nif \"processing\" not in st.session_state:\\n    st.session_state.processing = False\\n\\n# Flag for uploaded documents\\nif \"has_documents\" not in st.session_state:\\n    st.session_state.has_documents = False\\n\\n# Store uploaded file names\\nif \"uploaded_files\" not in st.session_state:\\n    st.session_state.uploaded_files = []\\n\\n# Flag to avoid duplicate document upload messages\\nif \"last_uploaded_file\" not in st.session_state:\\n    st.session_state.last_uploaded_file = None\\n\\n# Flag to control file uploader visibility\\nif \"show_uploader\" not in st.session_state:\\n    st.session_state.show_uploader = False\\n\\nchat_container = st.container()\\n\\n# Display previous messages\\nwith chat_container:\\n    for message in st.session_state.message_log:\\n        with st.chat_message(message[\"role\"]):\\n            content = message[\"content\"]\\n            think_matches = re.findall(r\\'<think>(.*?)</think>\\', content, flags=re.DOTALL)\\n            content_without_think = re.sub(r\\'<think>.*?</think>\\', \\'\\', content, flags=re.DOTALL)\\n\\n            st.markdown(content_without_think)\\n\\n            for think_text in think_matches:\\n                with st.expander(\"üí≠ Thought Process\"):\\n                    st.markdown(think_text)\\n    \\n    # Show processing indicator only when processing\\n    if st.session_state.processing:\\n        with st.chat_message(\"ai\"):\\n            st.write(\"Processing...\")\\n\\n# Display uploaded documents if any\\nif st.session_state.uploaded_files:\\n    with st.expander(\"üìö Uploaded Documents\"):\\n        for file_name in st.session_state.uploaded_files:\\n            st.write(f\"- {file_name}\")\\n\\n# Create a container for the upload button and chat input\\ninput_container = st.container()\\n\\nwith input_container:\\n    # Create two columns for the chat input and upload button\\n    col1, col2 = st.columns([10, 2])\\n    \\n    with col2:\\n        # Simple upload button instead of custom HTML/JS\\n        upload_button = st.button(\"üìÑ +\", key=\"upload_button\")\\n        if upload_button:\\n            st.session_state.show_uploader = not st.session_state.show_uploader\\n    \\n    with col1:\\n        # Chat input\\n        user_query = st.chat_input(\"Ask NOVA...\")\\n\\n# Show the file uploader when toggled\\nif st.session_state.show_uploader:\\n    uploaded_pdf = st.file_uploader(\\n        \"Upload PDF\",\\n        type=\"pdf\",\\n        key=\"pdf_uploader\"\\n    )\\n    \\n    # Process uploaded file\\n    if uploaded_pdf and (st.session_state.last_uploaded_file != uploaded_pdf.name):\\n        with st.spinner(\"Processing document...\"):\\n            num_chunks = process_pdf_file(uploaded_pdf)\\n            \\n            if num_chunks > 0:\\n                # Update state\\n                st.session_state.has_documents = True\\n                if uploaded_pdf.name not in st.session_state.uploaded_files:\\n                    st.session_state.uploaded_files.append(uploaded_pdf.name)\\n                \\n                # Add system message about the upload\\n                upload_message = f\"üìÑ Document \\'{uploaded_pdf.name}\\' successfully uploaded and processed ({num_chunks} chunks). You can now ask questions about this document.\"\\n                st.session_state.message_log.append({\"role\": \"ai\", \"content\": upload_message})\\n                \\n                # Update last uploaded file to prevent duplicate messages\\n                st.session_state.last_uploaded_file = uploaded_pdf.name\\n                \\n                # Hide the uploader after successful upload\\n                st.session_state.show_uploader = False\\n                \\n                # Rerun to update UI\\n                st.rerun()\\n            else:\\n                st.error(f\"Failed to process document \\'{uploaded_pdf.name}\\'. Please try again or use a different document.\")\\n\\n# Function to build the prompt chain\\ndef build_prompt_chain():\\n    # Start with just the system message\\n    messages = [SystemMessage(content=system_template)]\\n    \\n    # Add the conversation history\\n    for msg in st.session_state.message_log:\\n        if msg[\"role\"] == \"user\":\\n            messages.append(HumanMessage(content=msg[\"content\"]))\\n        elif msg[\"role\"] == \"ai\":\\n            messages.append(AIMessage(content=msg[\"content\"]))\\n    \\n    return messages\\n\\n# Function to determine if web search is needed (emotion-related patterns removed)\\ndef needs_web_search(query):\\n    # Check for explicit request for search\\n    explicit_search_patterns = [\\n        \"search for\", \"look up\", \"find information\", \"search the web\",\\n        \"what\\'s the latest\", \"current news\", \"recent updates\"\\n    ]\\n    \\n    query_lower = query.lower()\\n    \\n    for pattern in explicit_search_patterns:\\n        if pattern in query_lower:\\n            return True\\n    \\n    # Check for queries about current events, dates, or time-sensitive information\\n    time_sensitive_patterns = [\\n        \"today\", \"current\", \"latest\", \"recent\", \"now\", \"update\", \\n        \"news\", \"weather\", \"price\", \"stock\", \"bitcoin\", \"crypto\",\\n        \"happened\", \"trending\", \"score\", \"result\", \"happening\"\\n    ]\\n    \\n    for pattern in time_sensitive_patterns:\\n        if pattern in query_lower:\\n            return True\\n    \\n    # Check for questions about specific factual information that might need verification\\n    factual_patterns = [\\n        \"how many\", \"how much\", \"what is the population\", \"what is the distance\",\\n        \"how far\", \"how old\", \"when was\", \"where is\", \"who is the current\"\\n    ]\\n    \\n    for pattern in factual_patterns:\\n        if pattern in query_lower:\\n            return True\\n    \\n    # Don\\'t use web search for conversational queries\\n    conversational_patterns = [\\n        \"how are you\", \"what do you think\", \"can you help\",\\n        \"who are you\", \"tell me about yourself\"\\n    ]\\n    \\n    for pattern in conversational_patterns:\\n        if pattern in query_lower:\\n            return False\\n    \\n    # Default to not using web search for most queries\\n    return False\\n\\n# Function to determine if we should check documents (simplified)\\ndef needs_document_search(query):\\n    # Check for explicit request for document search\\n    document_patterns = [\\n        \"in the document\", \"from the pdf\", \"in the pdf\", \"document says\",\\n        \"check the document\", \"in the uploaded\", \"from the uploaded\",\\n        \"the document mentions\", \"in my document\", \"in my pdf\",\\n        \"what does the document say about\", \"find in document\",\\n        \"tell me about the document\", \"summarize the document\",\\n        \"what\\'s in the pdf\", \"what is in the document\",\\n        \"analyze the pdf\", \"analyze the document\"\\n    ]\\n    \\n    query_lower = query.lower()\\n    \\n    for pattern in document_patterns:\\n        if pattern in query_lower:\\n            return True\\n    \\n    # If there are documents and the query sounds like it needs information:\\n    if st.session_state.has_documents and len(st.session_state.uploaded_files) > 0:\\n        info_patterns = [\\n            \"what is\", \"how does\", \"tell me about\", \"explain\", \"summarize\",\\n            \"what are\", \"where is\", \"who is\", \"when did\", \"why did\",\\n            \"what was\", \"how many\", \"how much\"\\n        ]\\n        \\n        for pattern in info_patterns:\\n            if query_lower.startswith(pattern):\\n                return True\\n    \\n    # Always try document search if we have few documents\\n    if st.session_state.has_documents and len(st.session_state.uploaded_files) <= 3:\\n        return True\\n    \\n    # Default to not using document search unless explicitly requested\\n    return False\\n\\n# Handle user input\\nif user_query:\\n    # Add user message to chat history immediately\\n    st.session_state.message_log.append({\"role\": \"user\", \"content\": user_query})\\n    st.session_state.processing = True\\n    st.rerun()\\n\\n# Continue processing if in processing state\\nif st.session_state.processing:\\n    with st.spinner(\"\"):\\n        messages = build_prompt_chain()\\n        \\n        # Get the last user query\\n        last_user_query = st.session_state.message_log[-1][\"content\"]\\n        \\n        # Determine if web search or document search is needed\\n        should_search_web = needs_web_search(last_user_query)\\n        should_search_docs = needs_document_search(last_user_query)\\n        \\n        # Flag to track if we\\'ve already handled the query\\n        query_handled = False\\n        \\n        # Always try document search first if we have documents\\n        if st.session_state.has_documents:\\n            # Query documents\\n            document_results = query_documents(last_user_query)\\n            \\n            if \"No documents have been uploaded yet\" not in document_results and \"Error\" not in document_results:\\n                # Format document instruction with results\\n                doc_instruction = document_instruction_template.format(document_results=document_results)\\n                \\n                # Add the document results as context\\n                doc_context_message = SystemMessage(content=doc_instruction)\\n                \\n                # Create new messages list with the document context\\n                doc_messages = [\\n                    SystemMessage(content=system_template),\\n                    doc_context_message,\\n                    HumanMessage(content=f\"{last_user_query}\")\\n                ]\\n                \\n                # Get response from LLM with document results\\n                ai_response = llm_engine.invoke(doc_messages).content\\n                query_handled = True\\n                \\n                # Add a thought process about document search\\n                if \"<think>\" not in ai_response:\\n                    ai_response += f\"\\\\n\\\\n<think>Document search was performed and used to generate this response.</think>\"\\n        \\n        # Handle web search if needed and not already handled\\n        if should_search_web and not query_handled:\\n            # Perform web search\\n            search_results = perform_web_search(last_user_query)\\n            \\n            # Format search instruction with results\\n            search_instruction = search_instruction_template.format(search_results=search_results)\\n            \\n            # Add the search results as context\\n            web_context_message = SystemMessage(content=search_instruction)\\n            \\n            # Create new messages list with the search context\\n            search_messages = [\\n                SystemMessage(content=system_template),\\n                web_context_message,\\n                HumanMessage(content=f\"{last_user_query}\")\\n            ]\\n            \\n            # Get response from LLM with search results\\n            ai_response = llm_engine.invoke(search_messages).content\\n            query_handled = True\\n            \\n            # Add a thought process about web search\\n            if \"<think>\" not in ai_response:\\n                ai_response += f\"\\\\n\\\\n<think>Web search was performed and used to generate this response.</think>\"\\n        \\n        # If neither search was used or they didn\\'t provide useful results\\n        if not query_handled:\\n            # Add the current user query to the messages\\n            messages.append(HumanMessage(content=last_user_query))\\n            \\n            # Use LLM directly\\n            ai_response = llm_engine.invoke(messages).content\\n            \\n            # Add a thought process about using base knowledge\\n            if \"<think>\" not in ai_response:\\n                ai_response += f\"\\\\n\\\\n<think>No external search was performed. Response generated from base knowledge.</think>\"\\n\\n    # Add AI response to chat history\\n    st.session_state.message_log.append({\"role\": \"ai\", \"content\": ai_response})\\n    \\n    # Turn off processing state\\n    st.session_state.processing = False\\n    \\n    # Rerun to update the UI\\n    st.rerun()'}, {'path': 'prompts/templates.py', 'content': 'from config import CURRENT_DATE\\n\\n# Updated system message template\\nSYSTEM_TEMPLATE = f\"\"\"\\nYou are NOVA, an expert AI assistant with multi-document analysis and web search capabilities, created by KUMAR.\\n\\nToday\\'s date is {CURRENT_DATE}.\\n\\nCore capabilities:\\n1. Concise and accurate responses tailored to user needs\\n2. Real-time information retrieval via web search when necessary\\n3. Multi-Document analysis and question answering for various file types:\\n        - PDF documents\\n        - Word documents (DOCX, DOC)\\n        - Excel spreadsheets (XLSX, XLS)\\n        - CSV files\\n        - PowerPoint presentations (PPTX, PPT)\\n        - Text files (TXT, MD)\\n        - Data files (JSON, YAML)\\n        - HTML files\\n\\nWhen responding:\\n- If the user asks about current events, real-time data, or information you\\'re uncertain about, use the web search results\\n- If the user asks about uploaded documents, use the document query results\\n- ALWAYS use web search or document query results when provided to give accurate information\\n- Summarize information from search or document results in your own words\\n- Cite sources by referring to \"Source 1\", \"Source 2\", etc. or \"Document 1\", \"Document 2\", etc and provide their relevant links.\\n- NEVER claim you can\\'t access links, real-time data, or the internet\\n- When asked about dates, times, or current events, ALWAYS reference today\\'s date: {CURRENT_DATE}\\n- If search or document results are provided BUT they\\'re not relevant to the query, rely on your training instead\\n- If no search or document results are available, respond based on your training\\n\\nRemember: For most conversational queries, you don\\'t need web search or document queries. Use them only when necessary.\\n\\nIMPORTANT DOCUMENT HANDLING INSTRUCTIONS:\\n- When users upload documents, you CAN access their content through the Document Query tool\\n- NEVER say you cannot access document content - you have direct access to all uploaded documents\\n- When answering document questions, cite specific parts of the document\\n- If a document doesn\\'t contain relevant information, clearly state that instead of saying you can\\'t access it\\n- If the document doesn\\'t contain relevant information, use web search to answer the query, AND clearly state that you have used the websearch\\n- For structured data files (CSV, Excel, JSON), be aware of their tabular or hierarchical nature when providing information\\n- For presentations (PPT/PPTX), focus on the key points from each slide\\n\"\"\"\\n\\n# Enhanced search system message\\nSEARCH_INSTRUCTION_TEMPLATE = \"\"\"\\nIMPORTANT: You have access to recent web search results about the user\\'s query.\\n\\nThese search results may contain current information that you can use to answer the user\\'s question.\\n\\nPlease follow these guidelines:\\n1. Evaluate if the search results are RELEVANT to the user\\'s query\\n2. If relevant, synthesize information from the search results for an accurate, up-to-date answer\\n3. If NOT relevant, DO NOT use them and instead rely on your training\\n4. When using search results, cite your sources by referring to \"Source 1\", \"Source 2\", etc and provide their relevant links.\\n5. If the search results contain conflicting information, acknowledge this and present multiple perspectives\\n6. Present information in a clear, concise manner\\n7. NEVER claim you can\\'t access real-time data or the internet\\n\\nHere are the search results:\\n\\n{search_results}\\n\"\"\"\\n\\n# Document query system message\\nDOCUMENT_INSTRUCTION_TEMPLATE = \"\"\"\\nIMPORTANT: You have access to information from documents that the user has uploaded. The following content has been extracted from these documents.\\n\\nPlease follow these guidelines:\\n1. You CAN and SHOULD use this document information to answer the user\\'s query\\n2. NEVER say you cannot access document content - you have direct access shown below\\n3. If the information below doesn\\'t answer the query, state \"The uploaded documents don\\'t contain information about [specific topic]\" \\n4. If the uploaded documents does not contain information about [specific topic], use websearch to answer the query and provide relevant souce links\\n5. When using document information, cite your sources by referring to specific document names\\n6. Be specific about what parts of the documents you\\'re using\\n7. Present information in a clear, concise manner\\n8. Do not make up information that isn\\'t in the documents\\n\\nHere is the document information:\\n\\n{document_results}\\n\"\"\"'}, {'path': 'requirements.txt', 'content': 'langchain\\nlangchain-community\\nlangchain_groq\\nlangchain_ollama\\nlangchain_google_genai\\nlanggraph\\npython-dotenv\\nstreamlit\\ntavily-python\\npdfplumber\\nrequests\\nlangchain_cohere\\nunstructured \\npython-pptx \\npandas \\nopenpyxl \\npyyaml\\nlangchain-unstructured\\nmammoth'}, {'path': 'utils/document_utils.py', 'content': '# Updated document_utils.py\\nimport os\\nimport streamlit as st\\nfrom io import BytesIO\\nimport tempfile\\nimport pandas as pd\\nimport json\\nimport yaml\\nimport mammoth\\nfrom langchain_community.document_loaders import (\\n    PDFPlumberLoader,\\n    TextLoader,\\n    CSVLoader,\\n    JSONLoader,\\n    UnstructuredPowerPointLoader,\\n    UnstructuredExcelLoader,\\n    Docx2txtLoader,\\n    UnstructuredHTMLLoader,\\n    PyMuPDFLoader\\n)\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_core.vectorstores import InMemoryVectorStore\\nfrom config import CHUNK_SIZE, CHUNK_OVERLAP\\n\\ndef get_file_extension(filename):\\n    \"\"\"Get the file extension from a filename.\"\"\"\\n    return os.path.splitext(filename)[1].lower()\\n\\ndef process_document_file(uploaded_file, vector_store):\\n    \"\"\"Process various document file types and add them to the vector store\"\"\"\\n    # Get file extension\\n    file_extension = get_file_extension(uploaded_file.name)\\n    \\n    # Create a temporary file-like object\\n    temp_dir = tempfile.mkdtemp()\\n    temp_path = os.path.join(temp_dir, uploaded_file.name)\\n    \\n    try:\\n        # Write the file to disk temporarily\\n        with open(temp_path, \"wb\") as f:\\n            f.write(uploaded_file.getvalue())\\n            \\n        # Process based on file type\\n        raw_docs = []\\n        \\n        # PDF Files\\n        if file_extension in [\\'.pdf\\']:\\n            loader = PDFPlumberLoader(temp_path)\\n            raw_docs = loader.load()\\n            \\n        # Text Files\\n        elif file_extension in [\\'.txt\\', \\'.md\\', \\'.log\\']:\\n            loader = TextLoader(temp_path)\\n            raw_docs = loader.load()\\n            \\n        # Microsoft Word Documents\\n        elif file_extension in [\\'.docx\\']:\\n            loader = Docx2txtLoader(temp_path)\\n            raw_docs = loader.load()\\n            \\n        # Legacy Word Documents - requires extra handling\\n        elif file_extension in [\\'.doc\\']:\\n            # Convert DOC to DOCX-like format using mammoth\\n            with open(temp_path, \"rb\") as docx_file:\\n                result = mammoth.convert_to_html(docx_file)\\n                text = result.value\\n            \\n            # Create a document directly\\n            from langchain_core.documents import Document\\n            raw_docs = [Document(page_content=text, metadata={\"source\": uploaded_file.name})]\\n            \\n        # CSV Files\\n        elif file_extension in [\\'.csv\\']:\\n            loader = CSVLoader(temp_path)\\n            raw_docs = loader.load()\\n            \\n        # Excel Files\\n        elif file_extension in [\\'.xlsx\\', \\'.xls\\']:\\n            loader = UnstructuredExcelLoader(temp_path)\\n            raw_docs = loader.load()\\n            \\n        # PowerPoint Files\\n        elif file_extension in [\\'.ppt\\', \\'.pptx\\']:\\n            loader = UnstructuredPowerPointLoader(temp_path)\\n            raw_docs = loader.load()\\n            \\n        # JSON Files\\n        elif file_extension in [\\'.json\\']:\\n            # For JSON files, use a simple approach that works with various structures\\n            with open(temp_path, \\'r\\') as file:\\n                json_data = json.load(file)\\n            \\n            # Convert to string representation for simple processing\\n            text = json.dumps(json_data, indent=2)\\n            from langchain_core.documents import Document\\n            raw_docs = [Document(page_content=text, metadata={\"source\": uploaded_file.name})]\\n            \\n        # YAML Files\\n        elif file_extension in [\\'.yaml\\', \\'.yml\\']:\\n            # Similar approach for YAML files\\n            with open(temp_path, \\'r\\') as file:\\n                yaml_data = yaml.safe_load(file)\\n            \\n            # Convert to string representation\\n            text = yaml.dump(yaml_data, default_flow_style=False)\\n            from langchain_core.documents import Document\\n            raw_docs = [Document(page_content=text, metadata={\"source\": uploaded_file.name})]\\n            \\n        # HTML Files\\n        elif file_extension in [\\'.html\\', \\'.htm\\']:\\n            loader = UnstructuredHTMLLoader(temp_path)\\n            raw_docs = loader.load()\\n            \\n        else:\\n            # For unsupported file types, try a generic text loader as fallback\\n            try:\\n                loader = TextLoader(temp_path)\\n                raw_docs = loader.load()\\n            except Exception as e:\\n                raise ValueError(f\"Unsupported file type: {file_extension}\")\\n        \\n        # Store the raw document content for direct access\\n        full_text = \"\\\\n\\\\n\".join([doc.page_content for doc in raw_docs])\\n        st.session_state.document_contents[uploaded_file.name] = full_text\\n        \\n        # Get document metadata to display to user\\n        file_size = os.path.getsize(temp_path) / 1024  # size in KB\\n        num_pages = len(raw_docs)\\n        \\n        # Chunk documents\\n        text_processor = RecursiveCharacterTextSplitter(\\n            chunk_size=CHUNK_SIZE,\\n            chunk_overlap=CHUNK_OVERLAP,\\n            add_start_index=True\\n        )\\n        document_chunks = text_processor.split_documents(raw_docs)\\n        \\n        # Add metadata to track source document\\n        for chunk in document_chunks:\\n            if \"source\" not in chunk.metadata:\\n                chunk.metadata[\"source\"] = uploaded_file.name\\n            \\n            # Add file type for better context\\n            chunk.metadata[\"file_type\"] = file_extension.replace(\\'.\\', \\'\\')\\n        \\n        # Add to vector store\\n        vector_store.add_documents(document_chunks)\\n        \\n        # Return processing stats\\n        return {\\n            \"chunks\": len(document_chunks),\\n            \"file_size_kb\": round(file_size, 2),\\n            \"pages_or_sections\": num_pages,\\n            \"file_type\": file_extension.replace(\\'.\\', \\'\\').upper()\\n        }\\n        \\n    except Exception as e:\\n        raise Exception(f\"Error processing document: {str(e)}\")\\n        \\n    finally:\\n        # Clean up temporary files\\n        if os.path.exists(temp_path):\\n            os.remove(temp_path)\\n        try:\\n            os.rmdir(temp_dir)\\n        except:\\n            pass  # Directory might not be empty if other temp files were created\\n\\ndef query_documents(query: str) -> str:\\n    \"\"\"Query the vector store for relevant document chunks\"\"\"\\n    try:\\n        # Debug information\\n        doc_count = 0\\n        try:\\n            # This is a safer way to check document count that won\\'t crash if structure changes\\n            if hasattr(st.session_state.vector_store, \"_collection\"):\\n                doc_count = st.session_state.vector_store._collection.count()\\n            else:\\n                # Alternative method if _collection doesn\\'t exist\\n                doc_count = len(st.session_state.document_contents)\\n        except:\\n            doc_count = len(st.session_state.document_contents)\\n        \\n        # Check if there are documents in the vector store\\n        if doc_count == 0:\\n            return \"No documents have been uploaded yet. Please upload a document first to enable document queries.\"\\n        \\n        # Find related documents\\n        try:\\n            relevant_docs = st.session_state.vector_store.similarity_search(query, k=4)\\n        except Exception as e:\\n            # Fallback to direct document search if vector search fails\\n            if len(st.session_state.document_contents) > 0:\\n                fallback_results = []\\n                for doc_name, content in st.session_state.document_contents.items():\\n                    fallback_results.append(f\"Document: {doc_name}\\\\nContent: {content[:1000]}...\")\\n                \\n                context_text = \"\\\\n\\\\n\".join(fallback_results)\\n                \\n                # Log the issue but continue with fallback\\n                print(f\"Vector search failed, using fallback: {str(e)}\")\\n                \\n                return f\"Vector search failed, using direct document content.\\\\n\\\\n{context_text}\"\\n            else:\\n                return f\"Error searching documents: {str(e)}\"\\n        \\n        if not relevant_docs:\\n            # Fallback to direct document content search\\n            if len(st.session_state.document_contents) > 0:\\n                matching_docs = []\\n                for doc_name, content in st.session_state.document_contents.items():\\n                    # Simple keyword matching fallback\\n                    query_keywords = query.lower().split()\\n                    if any(keyword in content.lower() for keyword in query_keywords):\\n                        matching_docs.append(f\"Document: {doc_name}\\\\nContent: {content[:1000]}...\")\\n                \\n                if matching_docs:\\n                    context_text = \"\\\\n\\\\n\".join(matching_docs)\\n                    return context_text\\n                else:\\n                    return \"No relevant information found in the uploaded documents based on direct search.\"\\n            else:\\n                return \"No relevant information found in the uploaded documents.\"\\n        \\n        # Create context from documents with source tracking and file type\\n        doc_contexts = []\\n        for i, doc in enumerate(relevant_docs):\\n            source = doc.metadata.get(\"source\", f\"Document {i+1}\")\\n            file_type = doc.metadata.get(\"file_type\", \"unknown\").upper()\\n            doc_contexts.append(f\"Document: {source} (Type: {file_type})\\\\nContent: {doc.page_content}\")\\n        \\n        context_text = \"\\\\n\\\\n\".join(doc_contexts)\\n        \\n        return context_text\\n    except Exception as e:\\n        # Be more specific about the error and include debugging information\\n        error_message = f\"Error querying documents: {str(e)}\"\\n        print(error_message)  # For server logs\\n        \\n        # Include information about the document store state\\n        doc_info = \"No document information available\"\\n        if hasattr(st.session_state, \"document_contents\"):\\n            doc_names = list(st.session_state.document_contents.keys())\\n            doc_info = f\"Available documents: {\\', \\'.join(doc_names) if doc_names else \\'None\\'}\"\\n        \\n        return f\"{error_message}\\\\n{doc_info}\"\\n\\ndef needs_document_search(query):\\n    \"\"\"Determine if a document search is needed based on the query\"\"\"\\n    # Check for explicit request for document search\\n    document_patterns = [\\n        \"in the document\", \"from the file\", \"in the file\", \"document says\",\\n        \"check the document\", \"in the uploaded\", \"from the uploaded\",\\n        \"the document mentions\", \"in my document\", \"in my file\",\\n        \"what does the document say about\", \"find in document\",\\n        \"tell me about the document\", \"summarize the document\",\\n        \"what\\'s in the file\", \"what is in the document\",\\n        \"analyze the file\", \"analyze the document\"\\n    ]\\n    \\n    query_lower = query.lower()\\n    \\n    for pattern in document_patterns:\\n        if pattern in query_lower:\\n            return True\\n    \\n    # If there are documents and the query sounds like it needs information:\\n    if st.session_state.has_documents and len(st.session_state.uploaded_files) > 0:\\n        info_patterns = [\\n            \"what is\", \"how does\", \"tell me about\", \"explain\", \"summarize\",\\n            \"what are\", \"where is\", \"who is\", \"when did\", \"why did\",\\n            \"what was\", \"how many\", \"how much\"\\n        ]\\n        \\n        for pattern in info_patterns:\\n            if query_lower.startswith(pattern):\\n                return True\\n    \\n    # Always try document search if we have few documents\\n    if st.session_state.has_documents and len(st.session_state.uploaded_files) <= 3:\\n        return True\\n    \\n    # Default to not using document search unless explicitly requested\\n    return False\\n\\ndef get_supported_file_types():\\n    \"\"\"Return a list of supported file extensions\"\"\"\\n    return [\\n        \"pdf\", \"txt\", \"md\", \"docx\", \"doc\", \"csv\", \"xlsx\", \\n        \"xls\", \"ppt\", \"pptx\", \"json\", \"yaml\", \"yml\", \"html\", \"htm\", \"log\"\\n    ]'}, {'path': 'utils/search_utils.py', 'content': 'from tavily import TavilyClient\\nfrom config import TAVILY_API_KEY\\n\\n# Initialize Tavily client\\ntavily_client = TavilyClient(api_key=TAVILY_API_KEY)\\n\\ndef perform_web_search(query: str) -> str:\\n    \"\"\"Perform a web search using the Tavily API\"\"\"\\n    try:\\n        response = tavily_client.search(query, search_depth=\"advanced\", max_results=5)\\n        if response and \"results\" in response and len(response[\"results\"]) > 0:\\n            formatted_results = []\\n            for i, res in enumerate(response[\"results\"], 1):\\n                title = res.get(\\'title\\', \\'No title\\')\\n                url = res.get(\\'url\\', \\'#\\')\\n                content = res.get(\\'content\\', \\'No description available.\\')\\n                \\n                # Format the result with source number for easier reference\\n                formatted_results.append(f\"Source {i}: {title}\\\\nURL: {url}\\\\nContent: {content}\\\\n\")\\n            \\n            return \"\\\\n\".join(formatted_results)\\n        return \"No relevant search results found.\"\\n    except Exception as e:\\n        return f\"Error performing web search: {str(e)}\"\\n\\ndef needs_web_search(query):\\n    \"\"\"Determine if a web search is needed based on the query\"\"\"\\n    # Check for explicit request for search\\n    explicit_search_patterns = [\\n        \"search for\", \"look up\", \"find information\", \"search the web\",\\n        \"what\\'s the latest\", \"current news\", \"recent updates\"\\n    ]\\n    \\n    query_lower = query.lower()\\n    \\n    for pattern in explicit_search_patterns:\\n        if pattern in query_lower:\\n            return True\\n    \\n    # Check for queries about current events, dates, or time-sensitive information\\n    time_sensitive_patterns = [\\n        \"today\", \"current\", \"latest\", \"recent\", \"now\", \"update\", \\n        \"news\", \"weather\", \"price\", \"stock\", \"bitcoin\", \"crypto\",\\n        \"happened\", \"trending\", \"score\", \"result\", \"happening\"\\n    ]\\n    \\n    for pattern in time_sensitive_patterns:\\n        if pattern in query_lower:\\n            return True\\n    \\n    # Check for questions about specific factual information that might need verification\\n    factual_patterns = [\\n        \"how many\", \"how much\", \"what is the population\", \"what is the distance\",\\n        \"how far\", \"how old\", \"when was\", \"where is\", \"who is the current\"\\n    ]\\n    \\n    for pattern in factual_patterns:\\n        if pattern in query_lower:\\n            return True\\n    \\n    # Don\\'t use web search for conversational queries\\n    conversational_patterns = [\\n        \"how are you\", \"what do you think\", \"can you help\",\\n        \"who are you\", \"tell me about yourself\"\\n    ]\\n    \\n    for pattern in conversational_patterns:\\n        if pattern in query_lower:\\n            return False\\n    \\n    # Default to not using web search for most queries\\n    return False'}, {'path': 'utils/ui_utils.py', 'content': 'import streamlit as st\\n\\ndef apply_custom_styling():\\n    \"\"\"Apply custom styling to the Streamlit app\"\"\"\\n    st.markdown(\"\"\"\\n    <style>\\n    .stApp {\\n        background-color: #0E1117;\\n        color: #FFFFFF;\\n    }\\n    \\n    /* Chat Input Styling */\\n    .stChatInput input {\\n        background-color: #1E1E1E !important;\\n        color: #FFFFFF !important;\\n        border: 1px solid #3A3A3A !important;\\n    }\\n    \\n    /* User Message Styling */\\n    .stChatMessage[data-testid=\"stChatMessage\"]:nth-child(odd) {\\n        background-color: #1E1E1E !important;\\n        border: 1px solid #3A3A3A !important;\\n        color: #E0E0E0 !important;\\n        border-radius: 10px;\\n        padding: 15px;\\n        margin: 10px 0;\\n    }\\n    \\n    /* Assistant Message Styling */\\n    .stChatMessage[data-testid=\"stChatMessage\"]:nth-child(even) {\\n        background-color: #2A2A2A !important;\\n        border: 1px solid #404040 !important;\\n        color: #F0F0F0 !important;\\n        border-radius: 10px;\\n        padding: 15px;\\n        margin: 10px 0;\\n    }\\n    \\n    /* Avatar Styling */\\n    .stChatMessage .avatar {\\n        background-color: #00FFAA !important;\\n        color: #000000 !important;\\n    }\\n    \\n    /* Text Color Fix */\\n    .stChatMessage p, .stChatMessage div {\\n        color: #FFFFFF !important;\\n    }\\n    \\n    .stFileUploader {\\n        background-color: #1E1E1E;\\n        border: 1px solid #3A3A3A;\\n        border-radius: 5px;\\n        padding: 15px;\\n    }\\n    \\n    h1, h2, h3 {\\n        color: #00FFAA !important;\\n    }\\n    \\n    /* Style for upload button */\\n    .stButton button {\\n        background-color: #1E1E1E !important;\\n        color: #00FFAA !important;\\n        border: 1px solid #3A3A3A !important;\\n        border-radius: 5px;\\n        padding: 5px 10px;\\n        font-weight: bold;\\n        font-size: 16px;\\n    }\\n    \\n    .stButton button:hover {\\n        background-color: #2A2A2A !important;\\n        border: 1px solid #00FFAA !important;\\n    }\\n    \\n    /* Document list styling */\\n    .document-list {\\n        margin-top: 10px;\\n    }\\n    \\n    .document-item {\\n        padding: 8px;\\n        margin: 4px 0;\\n        background-color: #1E1E1E;\\n        border-radius: 4px;\\n        border-left: 3px solid #00FFAA;\\n    }\\n    \\n    /* File type badges */\\n    .file-badge {\\n        display: inline-block;\\n        padding: 2px 6px;\\n        border-radius: 4px;\\n        font-size: 12px;\\n        font-weight: bold;\\n        margin-left: 6px;\\n    }\\n    \\n    .file-badge-pdf {\\n        background-color: #FF5252;\\n        color: white;\\n    }\\n    \\n    .file-badge-docx {\\n        background-color: #2196F3;\\n        color: white;\\n    }\\n    \\n    .file-badge-xlsx {\\n        background-color: #4CAF50;\\n        color: white;\\n    }\\n    \\n    .file-badge-txt {\\n        background-color: #9E9E9E;\\n        color: white;\\n    }\\n    \\n    /* Custom document expander */\\n    .document-expander {\\n        border: 1px solid #3A3A3A;\\n        border-radius: 5px;\\n        margin-bottom: 15px;\\n    }\\n    </style>\\n    \"\"\", unsafe_allow_html=True)\\n\\ndef initialize_session_state():\\n    \"\"\"Initialize the session state variables\"\"\"\\n    if \"vector_store\" not in st.session_state:\\n        st.session_state.vector_store = None\\n    \\n    if \"document_contents\" not in st.session_state:\\n        st.session_state.document_contents = {}\\n    \\n    if \"message_log\" not in st.session_state:\\n        st.session_state.message_log = [\\n            {\"role\": \"ai\", \"content\": \"Hi! I\\'m NOVA, your AI assistant with multi-document intelligence. What can I do for you today? You can chat with me or upload documents (PDF, DOCX, CSV, Excel, PPT, TXT, and more) for analysis.\"}\\n        ]\\n    \\n    if \"processing\" not in st.session_state:\\n        st.session_state.processing = False\\n    \\n    if \"has_documents\" not in st.session_state:\\n        st.session_state.has_documents = False\\n    \\n    if \"uploaded_files\" not in st.session_state:\\n        st.session_state.uploaded_files = []\\n        \\n    if \"uploaded_file_info\" not in st.session_state:\\n        st.session_state.uploaded_file_info = {}\\n    \\n    if \"last_uploaded_file\" not in st.session_state:\\n        st.session_state.last_uploaded_file = None\\n    \\n    if \"show_uploader\" not in st.session_state:\\n        st.session_state.show_uploader = False\\n\\ndef display_file_badge(file_type):\\n    \"\"\"Generate HTML for a file type badge\"\"\"\\n    # Determine badge class based on file type\\n    badge_class = \"file-badge\"\\n    if file_type.lower() in [\"pdf\"]:\\n        badge_class += \" file-badge-pdf\"\\n    elif file_type.lower() in [\"docx\", \"doc\"]:\\n        badge_class += \" file-badge-docx\"\\n    elif file_type.lower() in [\"xlsx\", \"xls\", \"csv\"]:\\n        badge_class += \" file-badge-xlsx\"\\n    else:\\n        badge_class += \" file-badge-txt\"\\n        \\n    return f\\'<span class=\"{badge_class}\">{file_type}</span>\\''}, {'path': 'voicebot.py', 'content': '##########  NOVA WITH SAMPLE VOICE CAPABILITIES (NOT WORKING YET) ##########\\nimport os\\nimport streamlit as st\\nimport re\\nimport datetime\\nfrom dotenv import load_dotenv\\nfrom langchain_core.output_parsers import StrOutputParser\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom langchain_groq import ChatGroq\\nfrom langchain.tools import Tool\\nfrom langchain_core.runnables import RunnableBranch\\nfrom langchain_core.messages import AIMessage, HumanMessage, SystemMessage\\nfrom tavily import TavilyClient\\nfrom io import BytesIO\\nimport requests\\nimport base64\\n\\n# Document processing imports\\nfrom langchain_community.document_loaders import PDFPlumberLoader\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\nfrom langchain_core.vectorstores import InMemoryVectorStore\\nfrom langchain_ollama import OllamaEmbeddings\\n\\n# Load environment variables\\nload_dotenv()\\n\\n# Get API keys\\ngroq_api_key = os.getenv(\"GROQ_API_KEY\")\\ntavily_api_key = os.getenv(\"TAVILY_API_KEY\")\\nelevenlabs_api_key = os.getenv(\"ELEVENLABS_API_KEY\")\\n\\n# Initialize Tavily client\\ntavily_client = TavilyClient(api_key=tavily_api_key)\\n\\n# Initialize embedding model\\nEMBEDDING_MODEL = OllamaEmbeddings(model=\"deepseek-r1:1.5b\")\\n\\n# ElevenLabs configuration\\nELEVENLABS_API_URL = \"https://api.elevenlabs.io/v1\"\\nDEFAULT_VOICE_ID = \"21m00Tcm4TlvDq8ikWAM\"  # Rachel voice ID, you can change to your preferred voice\\n\\n# ElevenLabs Text-to-Speech function\\ndef text_to_speech(text, voice_id=DEFAULT_VOICE_ID):\\n    \"\"\"\\n    Convert text to speech using ElevenLabs API\\n    Returns audio data as base64 encoded string\\n    \"\"\"\\n    url = f\"{ELEVENLABS_API_URL}/text-to-speech/{voice_id}\"\\n    \\n    headers = {\\n        \"Accept\": \"audio/mpeg\",\\n        \"Content-Type\": \"application/json\",\\n        \"xi-api-key\": elevenlabs_api_key\\n    }\\n    \\n    data = {\\n        \"text\": text,\\n        \"model_id\": \"eleven_monolingual_v1\",\\n        \"voice_settings\": {\\n            \"stability\": 0.5,\\n            \"similarity_boost\": 0.5\\n        }\\n    }\\n    \\n    try:\\n        response = requests.post(url, json=data, headers=headers)\\n        \\n        if response.status_code == 200:\\n            # Convert binary audio data to base64 for embedding in HTML\\n            audio_data = base64.b64encode(response.content).decode()\\n            return audio_data\\n        else:\\n            print(f\"Error with ElevenLabs API: {response.status_code}\")\\n            print(response.text)\\n            return None\\n    except Exception as e:\\n        print(f\"Error with text to speech: {str(e)}\")\\n        return None\\n\\n# Function to create an audio player HTML element\\ndef get_audio_player_html(audio_data):\\n    \"\"\"\\n    Creates an HTML audio player for the provided base64 audio data\\n    \"\"\"\\n    audio_html = f\"\"\"\\n    <audio controls autoplay=\"true\" style=\"width: 100%;\">\\n        <source src=\"data:audio/mpeg;base64,{audio_data}\" type=\"audio/mpeg\">\\n        Your browser does not support the audio element.\\n    </audio>\\n    \"\"\"\\n    return audio_html\\n\\n# Apply custom styling\\nst.markdown(\"\"\"\\n    <style>\\n    .stApp {\\n        background-color: #0E1117;\\n        color: #FFFFFF;\\n    }\\n    \\n    /* Chat Input Styling */\\n    .stChatInput input {\\n        background-color: #1E1E1E !important;\\n        color: #FFFFFF !important;\\n        border: 1px solid #3A3A3A !important;\\n    }\\n    \\n    /* User Message Styling */\\n    .stChatMessage[data-testid=\"stChatMessage\"]:nth-child(odd) {\\n        background-color: #1E1E1E !important;\\n        border: 1px solid #3A3A3A !important;\\n        color: #E0E0E0 !important;\\n        border-radius: 10px;\\n        padding: 15px;\\n        margin: 10px 0;\\n    }\\n    \\n    /* Assistant Message Styling */\\n    .stChatMessage[data-testid=\"stChatMessage\"]:nth-child(even) {\\n        background-color: #2A2A2A !important;\\n        border: 1px solid #404040 !important;\\n        color: #F0F0F0 !important;\\n        border-radius: 10px;\\n        padding: 15px;\\n        margin: 10px 0;\\n    }\\n    \\n    /* Avatar Styling */\\n    .stChatMessage .avatar {\\n        background-color: #00FFAA !important;\\n        color: #000000 !important;\\n    }\\n    \\n    /* Text Color Fix */\\n    .stChatMessage p, .stChatMessage div {\\n        color: #FFFFFF !important;\\n    }\\n    \\n    .stFileUploader {\\n        background-color: #1E1E1E;\\n        border: 1px solid #3A3A3A;\\n        border-radius: 5px;\\n        padding: 15px;\\n    }\\n    \\n    h1, h2, h3 {\\n        color: #00FFAA !important;\\n    }\\n    \\n    /* Style for upload button */\\n    .stButton button {\\n        background-color: #1E1E1E !important;\\n        color: #00FFAA !important;\\n        border: 1px solid #3A3A3A !important;\\n        border-radius: 5px;\\n        padding: 5px 10px;\\n        font-weight: bold;\\n        font-size: 16px;\\n    }\\n    \\n    .stButton button:hover {\\n        background-color: #2A2A2A !important;\\n        border: 1px solid #00FFAA !important;\\n    }\\n    \\n    /* Audio player styling */\\n    audio {\\n        background-color: #1E1E1E;\\n        border-radius: 5px;\\n        width: 100%;\\n    }\\n    \\n    /* Toggle button styling */\\n    .toggle-button {\\n        background-color: #1E1E1E !important;\\n        color: #00FFAA !important;\\n        border: 1px solid #3A3A3A !important;\\n        border-radius: 5px;\\n        padding: 5px 10px;\\n        margin-right: 5px;\\n    }\\n    </style>\\n    \"\"\", unsafe_allow_html=True)\\n\\nst.title(\"NOVA\")\\nst.caption(\"Your AI Assistant with Document Intelligence and Voice\")\\n\\n# Initialize AI model (DeepSeek on Groq)\\nllm_engine = ChatGroq(model=\"Deepseek-R1-Distill-Qwen-32b\", groq_api_key=groq_api_key)\\n\\n# Initialize session state\\nif \"vector_store\" not in st.session_state:\\n    st.session_state.vector_store = InMemoryVectorStore(embedding=EMBEDDING_MODEL)\\n\\nif \"document_contents\" not in st.session_state:\\n    st.session_state.document_contents = {}\\n\\n# Add voice settings to session state\\nif \"voice_enabled\" not in st.session_state:\\n    st.session_state.voice_enabled = False\\n\\nif \"voice_id\" not in st.session_state:\\n    st.session_state.voice_id = DEFAULT_VOICE_ID\\n\\n# Document processing functions\\ndef process_pdf_file(uploaded_file):\\n    # Create a temporary file-like object\\n    pdf_file = BytesIO(uploaded_file.getvalue())\\n    \\n    # Save to a temporary file that PDFPlumberLoader can use\\n    temp_path = f\"temp_{uploaded_file.name}\"\\n    with open(temp_path, \"wb\") as f:\\n        f.write(pdf_file.getvalue())\\n        \\n    # Use the regular PDFPlumberLoader\\n    loader = PDFPlumberLoader(temp_path)\\n    try:\\n        raw_docs = loader.load()\\n        \\n        # Store the raw document content for direct access\\n        full_text = \"\\\\n\\\\n\".join([doc.page_content for doc in raw_docs])\\n        st.session_state.document_contents[uploaded_file.name] = full_text\\n        \\n        # Chunk documents\\n        text_processor = RecursiveCharacterTextSplitter(\\n            chunk_size=1000,\\n            chunk_overlap=200,\\n            add_start_index=True\\n        )\\n        document_chunks = text_processor.split_documents(raw_docs)\\n        \\n        # Add metadata to track source document\\n        for chunk in document_chunks:\\n            if \"source\" not in chunk.metadata:\\n                chunk.metadata[\"source\"] = uploaded_file.name\\n        \\n        # Add to vector store\\n        st.session_state.vector_store.add_documents(document_chunks)\\n        \\n        # Clean up the temporary file\\n        os.remove(temp_path)\\n        \\n        return len(document_chunks)\\n    except Exception as e:\\n        if os.path.exists(temp_path):\\n            os.remove(temp_path)\\n        st.error(f\"Error processing PDF: {str(e)}\")\\n        return 0\\n\\n# Define document RAG function with improved error handling and debugging\\ndef query_documents(query: str) -> str:\\n    try:\\n        # Debug information\\n        doc_count = 0\\n        try:\\n            # This is a safer way to check document count that won\\'t crash if structure changes\\n            if hasattr(st.session_state.vector_store, \"_collection\"):\\n                doc_count = st.session_state.vector_store._collection.count()\\n            else:\\n                # Alternative method if _collection doesn\\'t exist\\n                doc_count = len(st.session_state.document_contents)\\n        except:\\n            doc_count = len(st.session_state.document_contents)\\n        \\n        # Check if there are documents in the vector store\\n        if doc_count == 0:\\n            return \"No documents have been uploaded yet. Please upload a document first to enable document queries.\"\\n        \\n        # Find related documents\\n        try:\\n            relevant_docs = st.session_state.vector_store.similarity_search(query, k=4)\\n        except Exception as e:\\n            # Fallback to direct document search if vector search fails\\n            if len(st.session_state.document_contents) > 0:\\n                fallback_results = []\\n                for doc_name, content in st.session_state.document_contents.items():\\n                    fallback_results.append(f\"Document: {doc_name}\\\\nContent: {content[:1000]}...\")\\n                \\n                context_text = \"\\\\n\\\\n\".join(fallback_results)\\n                \\n                # Log the issue but continue with fallback\\n                print(f\"Vector search failed, using fallback: {str(e)}\")\\n                \\n                return f\"Vector search failed, using direct document content.\\\\n\\\\n{context_text}\"\\n            else:\\n                return f\"Error searching documents: {str(e)}\"\\n        \\n        if not relevant_docs:\\n            # Fallback to direct document content search\\n            if len(st.session_state.document_contents) > 0:\\n                matching_docs = []\\n                for doc_name, content in st.session_state.document_contents.items():\\n                    # Simple keyword matching fallback\\n                    query_keywords = query.lower().split()\\n                    if any(keyword in content.lower() for keyword in query_keywords):\\n                        matching_docs.append(f\"Document: {doc_name}\\\\nContent: {content[:1000]}...\")\\n                \\n                if matching_docs:\\n                    context_text = \"\\\\n\\\\n\".join(matching_docs)\\n                    return context_text\\n                else:\\n                    return \"No relevant information found in the uploaded documents based on direct search.\"\\n            else:\\n                return \"No relevant information found in the uploaded documents.\"\\n        \\n        # Create context from documents with source tracking\\n        doc_contexts = []\\n        for i, doc in enumerate(relevant_docs):\\n            source = doc.metadata.get(\"source\", f\"Document {i+1}\")\\n            doc_contexts.append(f\"Document: {source}\\\\nContent: {doc.page_content}\")\\n        \\n        context_text = \"\\\\n\\\\n\".join(doc_contexts)\\n        \\n        # Document QA prompt\\n        doc_qa_prompt = \"\"\"\\n        You are an expert research assistant. Use ONLY the provided document context to answer the query.\\n        If the answer is not in the provided context, state that you don\\'t have the information.\\n        Be clear, factual, and provide specific references to the document parts you\\'re using.\\n        \\n        Query: {query}\\n        Document Context: {context}\\n        \\n        Answer:\\n        \"\"\"\\n        \\n        prompt = ChatPromptTemplate.from_template(doc_qa_prompt)\\n        response_chain = prompt | llm_engine\\n        \\n        response = response_chain.invoke({\"query\": query, \"context\": context_text})\\n        \\n        # Add source information\\n        result = response.content\\n        if not result.strip().lower().startswith(\"i don\\'t have\"):\\n            result += f\"\\\\n\\\\n<think>Documents searched: {len(relevant_docs)} chunks from {\\', \\'.join(set([doc.metadata.get(\\'source\\', \\'Unknown\\') for doc in relevant_docs]))}</think>\"\\n        \\n        return result\\n    except Exception as e:\\n        # Be more specific about the error and include debugging information\\n        error_message = f\"Error querying documents: {str(e)}\"\\n        print(error_message)  # For server logs\\n        \\n        # Include information about the document store state\\n        doc_info = \"No document information available\"\\n        if hasattr(st.session_state, \"document_contents\"):\\n            doc_names = list(st.session_state.document_contents.keys())\\n            doc_info = f\"Available documents: {\\', \\'.join(doc_names) if doc_names else \\'None\\'}\"\\n        \\n        return f\"{error_message}\\\\n{doc_info}\"\\n\\n# Function to perform internet search\\ndef perform_web_search(query: str) -> str:\\n    try:\\n        response = tavily_client.search(query, search_depth=\"advanced\", max_results=5)\\n        if response and \"results\" in response and len(response[\"results\"]) > 0:\\n            formatted_results = []\\n            for i, res in enumerate(response[\"results\"], 1):\\n                title = res.get(\\'title\\', \\'No title\\')\\n                url = res.get(\\'url\\', \\'#\\')\\n                content = res.get(\\'content\\', \\'No description available.\\')\\n                \\n                # Format the result with source number for easier reference\\n                formatted_results.append(f\"Source {i}: {title}\\\\nURL: {url}\\\\nContent: {content}\\\\n\")\\n            \\n            return \"\\\\n\".join(formatted_results)\\n        return \"No relevant search results found.\"\\n    except Exception as e:\\n        return f\"Error performing web search: {str(e)}\"\\n\\n# Define Web Search and Document Query as Tools\\nweb_search_tool = Tool(\\n    name=\"Web Search\",\\n    func=perform_web_search,\\n    description=\"Use this tool to fetch the latest information from the web. Input a search query.\"\\n)\\n\\ndocument_query_tool = Tool(\\n    name=\"Document Query\",\\n    func=query_documents,\\n    description=\"Use this tool to search through uploaded documents. Input a search query.\"\\n)\\n\\n# Get current date\\ncurrent_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\\n\\n# Updated system message template with emotion-related content removed\\nsystem_template = f\"\"\"\\nYou are NOVA, an expert AI assistant with document analysis and web search capabilities, created by KUMAR.\\n\\nToday\\'s date is {current_date}.\\n\\nCore capabilities:\\n1. Concise and accurate responses tailored to user needs\\n2. Real-time information retrieval via web search when necessary\\n3. Document analysis and question answering for uploaded PDF documents\\n\\nWhen responding:\\n- If the user asks about current events, real-time data, or information you\\'re uncertain about, use the web search results\\n- If the user asks about uploaded documents, use the document query results\\n- ALWAYS use web search or document query results when provided to give accurate information\\n- Summarize information from search or document results in your own words\\n- Cite sources by referring to \"Source 1\", \"Source 2\", etc. or \"Document 1\", \"Document 2\", etc and provide their relevant links.\\n- NEVER claim you can\\'t access links, real-time data, or the internet\\n- When asked about dates, times, or current events, ALWAYS reference today\\'s date: {current_date}\\n- If search or document results are provided BUT they\\'re not relevant to the query, rely on your training instead\\n- If no search or document results are available, respond based on your training\\n\\nRemember: For most conversational queries, you don\\'t need web search or document queries. Use them only when necessary.\\n\\nIMPORTANT DOCUMENT HANDLING INSTRUCTIONS:\\n- When users upload documents, you CAN access their content through the Document Query tool\\n- NEVER say you cannot access document content - you have direct access to all uploaded documents\\n- When answering document questions, cite specific parts of the document\\n- If a document doesn\\'t contain relevant information, clearly state that instead of saying you can\\'t access it\\n- If the document doesn\\'t contain relevant information, use web search to answer the query, AND clearly state that you have used the websearch\\n\"\"\"\\n\\n# Enhanced search system message\\nsearch_instruction_template = \"\"\"\\nIMPORTANT: You have access to recent web search results about the user\\'s query.\\n\\nThese search results may contain current information that you can use to answer the user\\'s question.\\n\\nPlease follow these guidelines:\\n1. Evaluate if the search results are RELEVANT to the user\\'s query\\n2. If relevant, synthesize information from the search results for an accurate, up-to-date answer\\n3. If NOT relevant, DO NOT use them and instead rely on your training\\n4. When using search results, cite your sources by referring to \"Source 1\", \"Source 2\", etc and provide their relevant links.\\n5. If the search results contain conflicting information, acknowledge this and present multiple perspectives\\n6. Present information in a clear, concise manner\\n7. NEVER claim you can\\'t access real-time data or the internet\\n\\nHere are the search results:\\n\\n{search_results}\\n\"\"\"\\n\\n# Document query system message\\ndocument_instruction_template = \"\"\"\\nIMPORTANT: You have access to information from documents that the user has uploaded. The following content has been extracted from these documents.\\n\\nPlease follow these guidelines:\\n1. You CAN and SHOULD use this document information to answer the user\\'s query\\n2. NEVER say you cannot access document content - you have direct access shown below\\n3. If the information below doesn\\'t answer the query, state \"The uploaded documents don\\'t contain information about [specific topic]\" \\n4. If the uploaded documents does not contain information about [specific topic], use websearch to answer the query and provide relevant souce links\\n5. When using document information, cite your sources by referring to specific document names\\n6. Be specific about what parts of the documents you\\'re using\\n7. Present information in a clear, concise manner\\n8. Do not make up information that isn\\'t in the documents\\n\\nHere is the document information:\\n\\n{document_results}\\n\"\"\"\\n\\n# Initialize message log in session state\\nif \"message_log\" not in st.session_state:\\n    st.session_state.message_log = [\\n        {\"role\": \"ai\", \"content\": \"Hi! I\\'m NOVA, your AI assistant with document intelligence. What can I do for you today? You can chat with me or upload PDF documents for analysis.\"}\\n    ]\\n\\n# Flag for processing state\\nif \"processing\" not in st.session_state:\\n    st.session_state.processing = False\\n\\n# Flag for uploaded documents\\nif \"has_documents\" not in st.session_state:\\n    st.session_state.has_documents = False\\n\\n# Store uploaded file names\\nif \"uploaded_files\" not in st.session_state:\\n    st.session_state.uploaded_files = []\\n\\n# Flag to avoid duplicate document upload messages\\nif \"last_uploaded_file\" not in st.session_state:\\n    st.session_state.last_uploaded_file = None\\n\\n# Flag to control file uploader visibility\\nif \"show_uploader\" not in st.session_state:\\n    st.session_state.show_uploader = False\\n\\n# Create a sidebar for voice settings\\nwith st.sidebar:\\n    st.title(\"Voice Settings\")\\n    \\n    # Toggle for enabling/disabling voice\\n    voice_enabled = st.toggle(\"Enable Voice\", value=st.session_state.voice_enabled)\\n    if voice_enabled != st.session_state.voice_enabled:\\n        st.session_state.voice_enabled = voice_enabled\\n    \\n    # Voice selection dropdown (add more voices as needed)\\n    voice_options = {\\n        \"Rachel (Female)\": \"21m00Tcm4TlvDq8ikWAM\",\\n        \"Adam (Male)\": \"pNInz6obpgDQGcFmaJgB\",\\n        \"Sam (Male)\": \"yoZ06aMxZJJ28mfd3POQ\",\\n        \"Elli (Female)\": \"MF3mGyEYCl7XYWbV9V6O\"\\n    }\\n    \\n    selected_voice_name = st.selectbox(\\n        \"Select Voice\",\\n        options=list(voice_options.keys()),\\n        index=0\\n    )\\n    \\n    # Update voice ID in session state\\n    selected_voice_id = voice_options[selected_voice_name]\\n    if selected_voice_id != st.session_state.voice_id:\\n        st.session_state.voice_id = selected_voice_id\\n    \\n    # Voice settings\\n    if st.session_state.voice_enabled:\\n        st.info(\"üîä Voice responses are enabled\")\\n    else:\\n        st.info(\"üîá Voice responses are disabled\")\\n\\nchat_container = st.container()\\n\\n# Display previous messages\\nwith chat_container:\\n    for message in st.session_state.message_log:\\n        with st.chat_message(message[\"role\"]):\\n            content = message[\"content\"]\\n            think_matches = re.findall(r\\'<think>(.*?)</think>\\', content, flags=re.DOTALL)\\n            content_without_think = re.sub(r\\'<think>.*?</think>\\', \\'\\', content, flags=re.DOTALL)\\n\\n            st.markdown(content_without_think)\\n\\n            for think_text in think_matches:\\n                with st.expander(\"üí≠ Thought Process\"):\\n                    st.markdown(think_text)\\n    \\n    # Show processing indicator only when processing\\n    if st.session_state.processing:\\n        with st.chat_message(\"ai\"):\\n            st.write(\"Processing...\")\\n\\n# Display uploaded documents if any\\nif st.session_state.uploaded_files:\\n    with st.expander(\"üìö Uploaded Documents\"):\\n        for file_name in st.session_state.uploaded_files:\\n            st.write(f\"- {file_name}\")\\n\\n# Create a container for the upload button and chat input\\ninput_container = st.container()\\n\\nwith input_container:\\n    # Create two columns for the chat input and upload button\\n    col1, col2 = st.columns([10, 2])\\n    \\n    with col2:\\n        # Simple upload button instead of custom HTML/JS\\n        upload_button = st.button(\"üìÑ +\", key=\"upload_button\")\\n        if upload_button:\\n            st.session_state.show_uploader = not st.session_state.show_uploader\\n    \\n    with col1:\\n        # Chat input\\n        user_query = st.chat_input(\"Ask NOVA...\")\\n\\n# Show the file uploader when toggled\\nif st.session_state.show_uploader:\\n    uploaded_pdf = st.file_uploader(\\n        \"Upload PDF\",\\n        type=\"pdf\",\\n        key=\"pdf_uploader\"\\n    )\\n    \\n    # Process uploaded file\\n    if uploaded_pdf and (st.session_state.last_uploaded_file != uploaded_pdf.name):\\n        with st.spinner(\"Processing document...\"):\\n            num_chunks = process_pdf_file(uploaded_pdf)\\n            \\n            if num_chunks > 0:\\n                # Update state\\n                st.session_state.has_documents = True\\n                if uploaded_pdf.name not in st.session_state.uploaded_files:\\n                    st.session_state.uploaded_files.append(uploaded_pdf.name)\\n                \\n                # Add system message about the upload\\n                upload_message = f\"üìÑ Document \\'{uploaded_pdf.name}\\' successfully uploaded and processed ({num_chunks} chunks). You can now ask questions about this document.\"\\n                st.session_state.message_log.append({\"role\": \"ai\", \"content\": upload_message})\\n                \\n                # Update last uploaded file to prevent duplicate messages\\n                st.session_state.last_uploaded_file = uploaded_pdf.name\\n                \\n                # Hide the uploader after successful upload\\n                st.session_state.show_uploader = False\\n                \\n                # Rerun to update UI\\n                st.rerun()\\n            else:\\n                st.error(f\"Failed to process document \\'{uploaded_pdf.name}\\'. Please try again or use a different document.\")\\n\\n# Function to build the prompt chain\\ndef build_prompt_chain():\\n    # Start with just the system message\\n    messages = [SystemMessage(content=system_template)]\\n    \\n    # Add the conversation history\\n    for msg in st.session_state.message_log:\\n        if msg[\"role\"] == \"user\":\\n            messages.append(HumanMessage(content=msg[\"content\"]))\\n        elif msg[\"role\"] == \"ai\":\\n            messages.append(AIMessage(content=msg[\"content\"]))\\n    \\n    return messages\\n\\n# Function to determine if web search is needed (emotion-related patterns removed)\\ndef needs_web_search(query):\\n    # Check for explicit request for search\\n    explicit_search_patterns = [\\n        \"search for\", \"look up\", \"find information\", \"search the web\",\\n        \"what\\'s the latest\", \"current news\", \"recent updates\"\\n    ]\\n    \\n    query_lower = query.lower()\\n    \\n    for pattern in explicit_search_patterns:\\n        if pattern in query_lower:\\n            return True\\n    \\n    # Check for queries about current events, dates, or time-sensitive information\\n    time_sensitive_patterns = [\\n        \"today\", \"current\", \"latest\", \"recent\", \"now\", \"update\", \\n        \"news\", \"weather\", \"price\", \"stock\", \"bitcoin\", \"crypto\",\\n        \"happened\", \"trending\", \"score\", \"result\", \"happening\"\\n    ]\\n    \\n    for pattern in time_sensitive_patterns:\\n        if pattern in query_lower:\\n            return True\\n    \\n    # Check for questions about specific factual information that might need verification\\n    factual_patterns = [\\n        \"how many\", \"how much\", \"what is the population\", \"what is the distance\",\\n        \"how far\", \"how old\", \"when was\", \"where is\", \"who is the current\"\\n    ]\\n    \\n    for pattern in factual_patterns:\\n        if pattern in query_lower:\\n            return True\\n    \\n    # Don\\'t use web search for conversational queries\\n    conversational_patterns = [\\n        \"how are you\", \"what do you think\", \"can you help\",\\n        \"who are you\", \"tell me about yourself\"\\n    ]\\n    \\n    for pattern in conversational_patterns:\\n        if pattern in query_lower:\\n            return False\\n    \\n    # Default to not using web search for most queries\\n    return False\\n\\n# Function to determine if we should check documents (simplified)\\ndef needs_document_search(query):\\n    # Check for explicit request for document search\\n    document_patterns = [\\n        \"in the document\", \"from the pdf\", \"in the pdf\", \"document says\",\\n        \"check the document\", \"in the uploaded\", \"from the uploaded\",\\n        \"the document mentions\", \"in my document\", \"in my pdf\",\\n        \"what does the document say about\", \"find in document\",\\n        \"tell me about the document\", \"summarize the document\",\\n        \"what\\'s in the pdf\", \"what is in the document\",\\n        \"analyze the pdf\", \"analyze the document\"\\n    ]\\n    \\n    query_lower = query.lower()\\n    \\n    for pattern in document_patterns:\\n        if pattern in query_lower:\\n            return True\\n    \\n    # If there are documents and the query sounds like it needs information:\\n    if st.session_state.has_documents and len(st.session_state.uploaded_files) > 0:\\n        info_patterns = [\\n            \"what is\", \"how does\", \"tell me about\", \"explain\", \"summarize\",\\n            \"what are\", \"where is\", \"who is\", \"when did\", \"why did\",\\n            \"what was\", \"how many\", \"how much\"\\n        ]\\n        \\n        for pattern in info_patterns:\\n            if query_lower.startswith(pattern):\\n                return True\\n    \\n    # Always try document search if we have few documents\\n    if st.session_state.has_documents and len(st.session_state.uploaded_files) <= 3:\\n        return True\\n    \\n    # Default to not using document search unless explicitly requested\\n    return False\\n\\n# Handle user input\\nif user_query:\\n    # Add user message to chat history immediately\\n    st.session_state.message_log.append({\"role\": \"user\", \"content\": user_query})\\n    st.session_state.processing = True\\n    st.rerun()\\n\\n# Continue processing if in processing state\\nif st.session_state.processing:\\n    with st.spinner(\"\"):\\n        messages = build_prompt_chain()\\n        \\n        # Get the last user query\\n        last_user_query = st.session_state.message_log[-1][\"content\"]\\n        \\n        # Determine if web search or document search is needed\\n        should_search_web = needs_web_search(last_user_query)\\n        should_search_docs = needs_document_search(last_user_query)\\n        \\n        # Flag to track if we\\'ve already handled the query\\n        query_handled = False\\n        \\n        # Always try document search first if we have documents\\n        if st.session_state.has_documents:\\n            # Query documents\\n            document_results = query_documents(last_user_query)\\n            \\n            if \"No documents have been uploaded yet\" not in document_results and \"Error\" not in document_results:\\n                # Format document instruction with results\\n                doc_instruction = document_instruction_template.format(document_results=document_results)\\n                \\n                # Add the document results as context\\n                doc_context_message = SystemMessage(content=doc_instruction)\\n                \\n                # Create new messages list with the document context\\n                doc_messages = [\\n                    SystemMessage(content=system_template),\\n                    doc_context_message,\\n                    HumanMessage(content=f\"{last_user_query}\")\\n                ]\\n                \\n                # Get response from LLM with document results\\n                ai_response = llm_engine.invoke(doc_messages).content\\n                query_handled = True\\n                \\n                # Add a thought process about document search\\n                if \"<think>\" not in ai_response:\\n                    ai_response += f\"\\\\n\\\\n<think>Document search was performed and used to generate this response.</think>\"\\n        \\n        # Handle web search if needed and not already handled\\n        if should_search_web and not query_handled:\\n            # Perform web search\\n            search_results = perform_web_search(last_user_query)\\n            \\n            # Format search instruction with results\\n            search_instruction = search_instruction_template.format(search_results=search_results)\\n            \\n            # Add the search results as context\\n            web_context_message = SystemMessage(content=search_instruction)\\n            \\n            # Create new messages list with the search context\\n            search_messages = [\\n                SystemMessage(content=system_template),\\n                web_context_message,\\n                HumanMessage(content=f\"{last_user_query}\")\\n            ]\\n            \\n            # Get response from LLM with search results\\n            ai_response = llm_engine.invoke(search_messages).content\\n            query_handled = True\\n            \\n            # Add a thought process about web search\\n            if \"<think>\" not in ai_response:\\n                ai_response += f\"\\\\n\\\\n<think>Web search was performed and used to generate this response.</think>\"\\n        \\n        # If neither search was used or they didn\\'t provide useful results\\n        if not query_handled:\\n            # Add the current user query to the messages\\n            messages.append(HumanMessage(content=last_user_query))\\n            \\n            # Use LLM directly\\n            ai_response = llm_engine.invoke(messages).content\\n            \\n            # Add a thought process about using base knowledge\\n            if \"<think>\" not in ai_response:\\n                ai_response += f\"\\\\n\\\\n<think>No external search was performed. Response generated from base knowledge.</think>\"\\n\\n    # Add AI response to chat history\\n    st.session_state.message_log.append({\"role\": \"ai\", \"content\": ai_response})\\n    \\n    # Turn off processing state\\n    st.session_state.processing = False\\n    \\n    # Rerun to update the UI\\n    st.rerun()'}]}\n"
     ]
    }
   ],
   "source": [
    "# Example run (async)\n",
    "async def main():\n",
    "    initial_state = {\"owner\": \"kumar8074\", \"repo\": \"NOVA-AI\", \"branch\": \"main\"}\n",
    "    result = await flow.ainvoke(initial_state)\n",
    "    logger.info(\"Flow completed\")\n",
    "    # result contains final state including files\n",
    "    print(result)\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f3c8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surfEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
